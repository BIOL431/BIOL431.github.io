[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "",
    "text": "Welcome to BIOL 431"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to the Course",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  TEST",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n2 test\n\n1 + 1\n\n[1] 2\n\n\nasdfaf\n\n\n3 test2\nasfda"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "linearmodels.html",
    "href": "linearmodels.html",
    "title": "5  Linear Models",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n6 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n7 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "4  Simulation and Probability",
    "section": "",
    "text": "Schedule\n\nSimulating data (15 min)\nFunctions (10 min)\nWriting functions (25 min)\nBasics of probability distributions (20 min)\np-values (20 min)\n\nLearning outcomes\nAfter this lecture, students should know:\n\nSimulating data can help us understand what assumptions we’ve made and lets us build and test models when we know the answer\nFunctions are a set of code that take inputs and return outputs based on a set of instructions that the user gives\nProbability distributions can be discrete or continuous\nProbability distributions have different ranges (sets of possible values) and supports (values they can take)\nDiscrete probability distributions have probability mass, and those probabilities must sum to 1\nContinuous probability distributions have probability density under each potential value and the area under the curve must sum to 1\nThe normal (Gaussian) distribution has a range of \\(\\in (-\\infty, \\infty)\\) and support of all real numbers\nThe Bernoulli distribution has a range of \\(\\in (0, 1)\\) and a support of \\((0,1)\\)\nThe probability mass function for the Bernoulli distribution is \\(P(X=1) = p, P(X=0) = 1-p\\)\nReal numbers are those found on the number line (rational and irrational numbers) but not imaginary; integers are whole numbers that can be positive, negative, or zero; rational numbers can be expressed as ratio of two integers; irrational numbers cannot\nEmpirical data can be used to estimate probability of observing data\nProbability density or mass functions can be used to estimate the probability of observing data\nA p-value is the probability of observing your data, or data more extreme, if the null is true\n\n\n5 Why simulate data\n\n\n6 Functions\n\n\n7 Probability distributions\n\n\n8 Discrete\n\nrbinom(n = 1, size = 1, prob=0.5)\n\n[1] 1\n\n\n\n\n9 Continuous\n\nrnorm(1, 0, 1)\n\n[1] -0.601442\n\n\n\n\n10 Probability mass\n\npbinom(1, 1, 0.5)\n\n[1] 1\n\n\n\n\n11 Probability density\n\npnorm(1, 0, 1)\n\n[1] 0.8413447"
  },
  {
    "objectID": "jags.html",
    "href": "jags.html",
    "title": "18  Fitting Bayesian Models",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n19 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n20 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "modelfit.html",
    "href": "modelfit.html",
    "title": "6  Model Fitting",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n7 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n8 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "causality.html",
    "href": "causality.html",
    "title": "14  Correlation and Causation",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n15 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n16 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "hypotheses.html",
    "href": "hypotheses.html",
    "title": "2  Hypotheses and Models",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n3 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n4 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#test2",
    "href": "index.html#test2",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "test2",
    "text": "test2\n\n1 + 1\n\n[1] 2\n\n\n\ntest 3\n\nlevel 4"
  },
  {
    "objectID": "variables.html",
    "href": "variables.html",
    "title": "3  Random Variables",
    "section": "",
    "text": "Katie: HHHHL(L) Ed: L(H) David: (L) Rose: H(H) Jo: HLHHLLHHLHHLL(H)\nSchedule\n\nGuillemot chick survival (5 min) video\nSimulating draws from a Bernoulli distribution (15 min)\nActivity: probability of guessing horse or laminator correctly (30 min)\nBinomial distribution (10 min)\nLaw of large numbers (20 min)\nVectors (10 min)\n\nLearning outcomes\nAfter this lecture, students should know:\n\nRandom variables are described by probability distributions\nRandom variables can be described mathematically\nThe Bernoulli distribution simulates one flip of a coin\nThe probability of success is on a per trial basis\nThe Binomial distribution simulates multiple trials\nIncreasing sample sizes towards infinity results in more stable estimates of probability from observations\nR has built-in functions for simulating most probability distributions"
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "10  Probability Distributions",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n11 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n12 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "9  Uncertainty",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n10 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n11 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "pathanalysis.html",
    "href": "pathanalysis.html",
    "title": "15  Path Analysis",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n16 test\n\n1 + 1\n\n[1] 2\n\n\n\n\n17 test2\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#about-this-book-1",
    "href": "index.html#about-this-book-1",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "About this ‘book’",
    "text": "About this ‘book’\nThese are my notes to accompany lectures for BIOL 480Q/580Q: Ecological Modeling (Fall 2024). They are not meant to be a textbook by any means, but rather a pretty casual introduction to statistics and modeling in R relevant to ecologists. Because the course is cross-listed as a graduate and undergraduate course, I assume many folks are coming to it with minimal background in probability, statistics, or coding, so this should hopefully be a pretty gentle introduction.\nAlso, I am writing it up as we go along in the course this semester, so more sections will be added as we go along. I will do my best to tell you which section(s) we will cover each week but we may spill over if we get through something quickly, or spend more time on some concepts than I anticipate."
  },
  {
    "objectID": "index.html#why-this-course",
    "href": "index.html#why-this-course",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "Why this course?",
    "text": "Why this course?\nJust before I started graduate school nearly a decade ago, a survey found that most early career ecologists lacked adequate quantitative training and 75% were not satisfied with their own understanding of ecological modeling. Ecology has always been heavily rooted in mathematics, and has become increasingly quantitative in recent decades, but graduate training has lagged behind that trend. I was lucky to have taken an Ecological Modeling course taught by two brilliant quantitative ecologists (and excellent human beings) - Morgan Tingley and Robi Bagchi - during my PhD at the University of Connecticut. This course is inspired by theirs, and similarly aims to train the next generation of quantitative ecologists.\n\n\n\nFigure from Barraquand et al. (2014) showing the relationship between early career ecologists’ satisfaction with their understanding of mathematical models and how involved they are in ecological modeling.\n\n\nOne of the reasons I teach this course is because I think it is really important for students to really understand their data and how to properly analyze it to address their hypotheses. Too often, the way statistics is taught (including how I was first introduced to it as an undergrad) is very rigid, with strict rules to follow, assumptions to be met, and prescribed ways of doing things. Many of us have come across guidelines like these charts for how to pick the right statistical test for your data.\n \nI also think that this approach to learning statistics instills a wariness about doing analyses ‘wrong’, which can turn many students away from statistics and modeling. I have absolutely no data to back this up, but I suspect the rigid way in which statistics is taught is one reason for a gender gap in quantitative ecology. Gender stereotypes can cause girls to internalize that they are ‘bad at math’ at an early age, which combined with a fear of not getting the ‘rules’ of statistics right, could be one reason why only 4% of early career women in ecology indicate being very involved in ecological modeling compared to 10% of men.\n\n\n\nGender differences in how PhD students and postdocs in North America rate their involvement in ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014).\n\n\n\n\n\nGender differences in how PhD students and postdocs in North America rate their level of comfort in using equations for ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014).\n\n\nIt is also really important for students to develop quantitative skills for their future employment prospects. For graduate students aiming for academic careers, quantitative skills can help them secure permanent positions. 40% of faculty job listings in ecology and related fields require some level of quantitative skills, and 21% require “strong” quantitative skills. Many industry and government research positions also require quantitative skills, including data analysis, visualization, and modeling.\nPerhaps most importantly, learning how to code and build models opens up a whole new world of possibilities for asking interesting questions. More often than not, we address questions using the tools we have at our disposal (i.e. if you’ve got a hammer, everything looks like a nail). The type of statistical tests that students often learn are inadequate to deal with many of the questions and hypotheses in ecology and related fields. Instead of being constrained by what statistical tests will be possible with the data or cramming the data into a test that does not really fit, students can learn to build models that truly address their research questions, opening up the possibility for more novel hypotheses."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "About me",
    "text": "About me\nHi! If you’re reading this, you’re probably a student in BIOL 431/531: Ecological Modeling. If not, it’s a bit weird that you ended up here, but welcome. I’m Eliza Grames, an Assistant Professor at Binghamton University. I am a conservation biologist and quantitative ecologist whose research primarily focuses on understanding the long-term status and trends of insect biodiversity, drivers of insect biodiversity loss, and consequences of declines in insect abundance and biomass for ecosystem function. As the course goes on, you will also learn that I really like cooking, the Lord of the Rings, and memes."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "About this ‘book’",
    "text": "About this ‘book’\nThese notes are meant to supplement and accompany lectures and labs for BIOL 431/531: Ecological Modeling (Spring 2026) at Binghamton University. They are not meant to be a formal textbook by any means, but rather a pretty casual introduction to data, statistics, and modeling in R for ecologists. Because the course is cross-listed as a graduate and undergraduate course, I assume most people have minimal background in probability, statistics, or coding, so this should hopefully be a pretty gentle (and meme-heavy) introduction.\nThis ‘book’ was first drafted in Fall 2024, however, the course was previously taught as a 2-credit graduate seminar so I am editing and writing new sections as we go along. More sections will be added, and current sections may be updated depending on the pacing of the course (e.g., we may slow down if topics are particularly challenging, or may breeze through some if they do not warrant more time). This is the first semester the course has been taught as a full course with a lab section, and there will be some growing pains. I will do my best to tell you which section(s) we will cover each week and they will roughly parallel the syllabus, but we could end up skipping ahead, going backwards, or spending more time on some concepts than I anticipate.\nIf you’re reading along and find something to be particularly confusing, please do let me know! This is one of the two courses I regularly teach (the other being BIOL 477: Conservation Biology) and I will be updating this ‘book’ and all the other course materials every semester until it is finally in a shape I am mostly happy with (at which point I will still be tweaking things, but for the first few semesters, there will likely be major changes)."
  },
  {
    "objectID": "index.html#why-do-i-teach-this-course",
    "href": "index.html#why-do-i-teach-this-course",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "Why do I teach this course?",
    "text": "Why do I teach this course?\nEcology has always been heavily rooted in mathematics, but we often don’t include it in student training as much as we should. Just before I started graduate school (a decade ago, at this point!), a survey found that most early career ecologists lacked adequate quantitative training and 75% were not satisfied with their own understanding of ecological modeling. Ecology has always been heavily rooted in mathematics, and has become increasingly quantitative in recent decades, but graduate training has lagged behind that trend. I was lucky to have taken an Ecological Modeling course taught by two brilliant quantitative ecologists (and excellent human beings) - Morgan Tingley and Robi Bagchi - during my PhD at the University of Connecticut. This course is inspired by theirs, and similarly aims to train the next generation of quantitative ecologists.\n\n\n\nFigure from Barraquand et al. (2014) showing the relationship between early career ecologists’ satisfaction with their understanding of mathematical models and how involved they are in ecological modeling.\n\n\nOne of the reasons I teach this course is because I think it is really important for students to really understand their data and how to properly analyze it to address their hypotheses. Too often, the way statistics is taught (including how I was first introduced to it as an undergrad) is very rigid, with strict rules to follow, assumptions to be met, and prescribed ways of doing things. Many of us have come across guidelines like these charts for how to pick the right statistical test for your data.\n \nI also think that this approach to learning statistics instills a wariness about doing analyses ‘wrong’, which can turn many students away from statistics and modeling. I have absolutely no data to back this up, but I suspect the rigid way in which statistics is taught is one barrier to more students from historically underrepresented groups in STEM pursuing quantitative ecology. For example, gender stereotypes can cause girls to internalize that they are ‘bad at math’ at an early age, which combined with a fear of not getting the ‘rules’ of statistics right, could be one reason why only 4% of early career women in ecology indicate being very involved in ecological modeling compared to 10% of men. Note that the survey these data are based on only included binary categories for gender (one of the many ways in which data, and statistics, we encounter do not always reflect reality!).\n\n\n\nGender differences in how PhD students and postdocs in North America rate their involvement in ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014).\n\n\n\n\n\nGender differences in how PhD students and postdocs in North America rate their level of comfort in using equations for ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014).\n\n\nIt is also really important for students to develop quantitative skills for their future employment prospects. For graduate students aiming for academic careers, quantitative skills can help them secure permanent positions. 40% of faculty job listings in ecology and related fields require some level of quantitative skills, and 21% require “strong” quantitative skills. Many industry and government research positions also require quantitative skills, including data analysis, visualization, and modeling.\nPerhaps most importantly, learning how to code and build models opens up a whole new world of possibilities for asking interesting questions. More often than not, we address questions using the tools we have at our disposal (i.e. if you’ve got a hammer, everything looks like a nail). The type of statistical tests that students often learn are inadequate to deal with many of the questions and hypotheses in ecology and related fields. Instead of being constrained by what statistical tests will be possible with the data or cramming the data into a test that does not really fit, students can learn to build models that truly address their research questions, opening up the possibility for more novel hypotheses."
  },
  {
    "objectID": "index.html#how-to-approach-this-course",
    "href": "index.html#how-to-approach-this-course",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "How to approach this course",
    "text": "How to approach this course\nThe goal of this course is to get you to think quantitatively i.e. recognizing that data are generated by deterministic and stochastic processes, both of which are defined by you (or your understanding of the biology of the system that you are modeling or making inference on). No one can really tell you how to model or analyze your data - there is no lookup table for ecological modeling.\nAfter this course, you should know enough to 1) be dangerous on Google / StackOverflow to find analytical / modeling approaches that match your questions and data, and 2) write or adapt code to implement those models and not be reliant on a GUI. The goal is not complete mastery of statistics, modeling, or data analysis.\n\n\n\nThe goal of this course is to get you closer to becoming dangerous.\n\n\n1. It is not about content mastery. This course likely requires a different mindset from what you have encountered previously, primarily because it is not about content mastery. To be a specialist in any field, including biology, there is a lot of content that you need to have under your belt, which is why many undergraduate (and graduate) courses emphasize understanding principles and knowing certain key information (e.g. you need to know population dynamics, cell division, anatomy and physiology, evolutionary principles, etc.). In contrast, this course is much more about building your toolset and learning techniques, then recombining them in new ways when faced with different modeling problems. In many ways, learning how to analyze your data, how to code, how to specify models, etc. is like learning how to cook.\nThink of content mastery as foraging - if you are in the woods collecting mushrooms, you must know which ones are edible and which are poisonous and how to tell the difference. Knowing what you can and cannot eat is essential background information. Modeling, on the other hand, is more like cooking - what are you going to do with what you’ve gathered?\n2. It is better to know of many methods than to know a few in-depth.\nWe have 28 hours together this semester. With that time, I could teach you one specific model each week and we could go very in-depth to know all the details, assumptions, modifications, etc. To put it in a cooking context, I could teach you one recipe each week and by the end of the course you would know how to make 14 of my favorite dishes. Or, instead, you could be exposed to many different types of food that you might like to make depending on what you’re in the mood for, covering a variety of cooking styles, techniques, and cuisines. Knowing that these dishes exist and having a general sense of what you want to make gives you many more options than just knowing how I make guacamole.\n3. Not all methods are suitable for your research questions.\nEven knowing that there are many dishes you could make, not all of them will suit you. Some weeks the dishes won’t work for your dietary needs (e.g. maybe you never have nested data) or you will wonder why anyone would ever want to cook French food. Then, we will get to Mexican food the next week (er, I mean structural equation modeling) and it will be exactly what you were looking for. Even on weeks that are not currently relevant to your research, it is still good to know that they exist and you may encounter those methods in other ways and be glad you have a basic understanding (e.g. as a peer reviewer).\n4. You can always look up how to implement a method.\nIf you know a dish exists but you are not entirely sure how to make it, you can look up a recipe and follow along. The same is true of models. Some recipes are not exactly what you want, but if you know what you’re looking for, eventually you’ll find a good example, guide, or tutorial. And sometimes, you may go to a restaurant (i.e. a journal article) and you really like it and try to recreate it!\n5. Practicing is important.\nThe first time you try a new recipe, it may not go well, but you’ll get better with practice. And at least you tried! With coding, you will need to keep at it and you will learn new tricks the more you practice.\n6. Selecting methods a priori is better than post-hoc.\nSometimes, you have been handed ingredients and are trying to find a recipe that will work with what you have. For example, in a data context, your PI may give you a dataset, you may be working with historical data, or you collected the data before having a plan for analysis. In an ideal world, you know what you are going to cook in advance so you have all the ingredients that you need on hand.\n\n\n\nFiguring out which models are possible post-hoc.\n\n\n\n\n\nPlanning your analyses before collecting data,"
  },
  {
    "objectID": "index.html#a-brief-history-of-statistics-and-eugenics",
    "href": "index.html#a-brief-history-of-statistics-and-eugenics",
    "title": "BIOL 431/531: Ecological Modeling",
    "section": "A brief history of statistics and eugenics",
    "text": "A brief history of statistics and eugenics\nThere are many figures in ecology and evolutionary biology that were giants in their field and helped shape modern science, and who were also extremely problematic individuals (and not just because ‘it was a different time’). Often, the way we learn about these individuals is first through their contributions to the field, and secondarily that they were a ‘bad person’ but that we should still value their intellectual contributions and somehow consider those to be separate from the person. For example, John James Audubon made numerous contributions to ornithology, and also bought and sold enslaved people. You could argue that those are separate aspects of the same person (…you could also argue that he greatly benefited from his status in society as a result of oppressing and enslaving other people and that enabled him to contribute to ornithology, but that is a conversation for a different course…). This is not the case with statistics. Most of modern statistics is built on work done by Sir Francis Galton, Karl Pearson, and Ronald Fisher, all of whom were staunch advocates of eugenics. They also collectively developed the ideas of standard deviation, correlation, regression to the mean, the correlation coefficient, method of moments, \\(\\chi^2\\) test, p-values, principle components analysis, and many other fundamental ideas in statistics. The point is not that these men made exceptional contributions to statistics and also happened to be eugenicists, but rather that they developed statistics to support their eugenicist viewpoints. After all, who could argue with the data they showed to support their view that other races were inferior to white Europeans?\n\n\n\nKarl Pearson (left) and Sir Francis Galton (right), circa 1910\n\n\nMany of the concepts we still use to this day in statistics come out of this tradition. For example, even the idea that a population (in the statistical sense) can be described from a single distribution is rooted in eugenics. At the same time Pearson was writing about mixtures of ‘homogeneous groups’ from a mathematical perspective, he was also advocating for colonialism and destruction of what he considered to be ‘inferior races’ because a more ‘homogeneous’ society was better.\n\n\n\nPearson in Contributions to the Mathematical Theory of Evolution in 1894\n\n\n\n\n\nPearson in National Life from the Standpoint of Science\n\n\nThe specific values, thresholds, cutoffs, and conventions that underlie most of null hypothesis significance testing were developed to support a eugenicist agenda. Always remember that they are not mathematically justified or ordained by some law of nature, but rather prescribed by a handful of like-minded eugenicists over a century ago. You should not feel bad about “breaking” their rules from time to time. Also, statistics is constantly evolving, so there are no hard and fast rules to follow."
  },
  {
    "objectID": "intro.html#how-to-approach-this-course",
    "href": "intro.html#how-to-approach-this-course",
    "title": "1  Thinking like a Modeler",
    "section": "1.1 How to approach this course",
    "text": "1.1 How to approach this course\nLearning how to code and model is a lot like learning how to cook in many ways. You can be pretty bad at it and still do it, it takes a lot of time to get good at it, and there are many, many different ways to be good at it (and a whole lot of different and often exciting or truly horrendous ways to be bad at it!).\n1. It is not about content mastery. This course likely requires a different mindset from what you have encountered previously, primarily because it is not about content mastery. To be a specialist in any field, including biology, there is a lot of content that you need to have under your belt, which is why many undergraduate (and graduate) courses emphasize understanding principles and knowing certain key information (e.g. you need to know population dynamics, cell division, anatomy and physiology, evolutionary principles, etc.). In contrast, this course is much more about building your toolset and learning techniques, then recombining them in new ways when faced with different modeling problems. In many ways, learning how to analyze your data, how to code, how to specify models, etc. is like learning how to cook.\nThink of content mastery as foraging - if you are in the woods collecting mushrooms, you must know which ones are edible and which are poisonous and how to tell the difference. Knowing what you can and cannot eat is essential background information. Modeling, on the other hand, is more like cooking - what are you going to do with what you’ve gathered?\n2. It is better to know of many methods than to know a few in-depth.\nWe have 84 hours together this semester. With that time, I could teach you one specific model each week and we could go very in-depth to know all the details, assumptions, modifications, etc. To put it in a cooking context, I could teach you one recipe each week and by the end of the course you would know how to make 14 of my favorite dishes. Or, instead, you could be exposed to many different types of food that you might like to make depending on what you’re in the mood for, covering a variety of cooking styles, techniques, and cuisines. Knowing that these dishes exist and having a general sense of what you want to make gives you many more options than just knowing how I make guacamole.\n3. Not all methods are suitable for your research questions.\nEven knowing that there are many dishes you could make, not all of them will suit you. Some weeks the dishes won’t work for your dietary needs (e.g. maybe you never have nested data) or you will wonder why anyone would ever want to cook French food. Then, we will get to Mexican food the next week (er, I mean Principal Components Analysis) and it will be exactly what you were looking for. Even on weeks that are not currently relevant to your research, it is still good to know that they exist and you may encounter those methods in other ways and be glad you have a basic understanding (e.g. as a peer reviewer, or food critic).\n4. You can always look up how to implement a method.\nIf you know a dish exists but you are not entirely sure how to make it, you can look up a recipe and follow along. The same is true of models. Some recipes are not exactly what you want, but if you know what you’re looking for, eventually you’ll find a good example, guide, or tutorial. And sometimes, you may go to a restaurant (i.e. a journal article) and taste something you really like it and try to recreate it!\n5. Practicing is important.\nThe first time you try a new recipe, it may not go well, but you’ll get better with practice. And at least you tried! With coding, you will need to keep at it and you will learn new tricks the more you practice.\n6. Selecting methods a priori is better than post-hoc.\nSometimes, you have been handed ingredients and are trying to find a recipe that will work with what you have. For example, in a data context, your PI may give you a dataset, you may be working with historical data, or you collected the data before having a plan for analysis. In an ideal world, you know what you are going to cook in advance so you have all the ingredients that you need on hand."
  },
  {
    "objectID": "intro.html#a-brief-history-of-statistics-and-eugenics",
    "href": "intro.html#a-brief-history-of-statistics-and-eugenics",
    "title": "2  Thinking like a Modeler",
    "section": "2.3 A brief history of statistics and eugenics",
    "text": "2.3 A brief history of statistics and eugenics\nThere are many figures in ecology and evolutionary biology that were giants in their field and helped shape modern science, and who were also extremely problematic individuals (and not just because ‘it was a different time’). Often, the way we learn about these individuals is first through their contributions to the field, and secondarily that they were a ‘bad person’ but that we should still value their intellectual contributions and somehow consider those to be separate from the person. For example, John James Audubon made numerous contributions to ornithology, and also bought and sold enslaved people. You could argue that those are separate aspects of the same person (…you could also argue that he greatly benefited from his status in society as a result of oppressing and enslaving other people and that enabled him to contribute to ornithology, but that is a conversation for a different course…). This is not the case with statistics. Most of modern statistics is built on work done by Sir Francis Galton, Karl Pearson, and Ronald Fisher, all of whom were staunch advocates of eugenics. They also collectively developed the ideas of standard deviation, correlation, regression to the mean, the correlation coefficient, method of moments, \\(\\chi^2\\) test, p-values, principle components analysis, and many other fundamental ideas in statistics. The point is not that these men made exceptional contributions to statistics and also happened to be eugenicists, but rather that they developed statistics to support their eugenicist viewpoints. After all, who could argue with the data they showed to support their view that other races were inferior to white Europeans?\n\n\n\nKarl Pearson (left) and Sir Francis Galton (right), circa 1910\n\n\nMany of the concepts we still use to this day in statistics come out of this tradition. For example, even the idea that a population (in the statistical sense) can be described from a single distribution is rooted in eugenics. At the same time Pearson was writing about mixtures of ‘homogeneous groups’ from a mathematical perspective, he was also advocating for colonialism and destruction of what he considered to be ‘inferior races’ because a more ‘homogeneous’ society was better.\n\n\n\nPearson in Contributions to the Mathematical Theory of Evolution in 1894\n\n\n\n\n\nPearson in National Life from the Standpoint of Science\n\n\nThe specific values, thresholds, cutoffs, and conventions that underlie most of null hypothesis significance testing were developed to support a eugenicist agenda. Always remember that they are not mathematically justified or ordained by some law of nature, but rather prescribed by a handful of like-minded eugenicists over a century ago. You should not feel bad about “breaking” their rules from time to time. Also, statistics is constantly evolving, so there are no hard and fast rules to follow."
  },
  {
    "objectID": "lessonplans.html#inference",
    "href": "lessonplans.html#inference",
    "title": "Lesson Plans",
    "section": "Inference",
    "text": "Inference\n\nIntroduction to the course\n\n\nHypotheses and models"
  },
  {
    "objectID": "lessonplans.html#random-variables",
    "href": "lessonplans.html#random-variables",
    "title": "Lesson Plans",
    "section": "Random variables",
    "text": "Random variables\n\nRandom variables\n\n\nSimulation and probability"
  },
  {
    "objectID": "lessonplans.html#linear-models-i",
    "href": "lessonplans.html#linear-models-i",
    "title": "Lesson Plans",
    "section": "Linear models I",
    "text": "Linear models I\n\nComponents of linear models\n\n\nModel fitting"
  },
  {
    "objectID": "lessonplans.html#linear-models-ii",
    "href": "lessonplans.html#linear-models-ii",
    "title": "Lesson Plans",
    "section": "Linear models II",
    "text": "Linear models II\n\nEvaluating models\n\n\nPrediction and inference"
  },
  {
    "objectID": "lessonplans.html#linear-models-ii-1",
    "href": "lessonplans.html#linear-models-ii-1",
    "title": "Lesson Plans",
    "section": "Linear models II",
    "text": "Linear models II\n\nUncertainty\n\n\nProbability distributions"
  },
  {
    "objectID": "lessonplans.html#generalized-linear-models",
    "href": "lessonplans.html#generalized-linear-models",
    "title": "Lesson Plans",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nLink functions\n\n\nDistributions"
  },
  {
    "objectID": "lessonplans.html#mixed-effects-models",
    "href": "lessonplans.html#mixed-effects-models",
    "title": "Lesson Plans",
    "section": "Mixed effects models",
    "text": "Mixed effects models\n\nNon-independence\n\n\nRandom effects"
  },
  {
    "objectID": "lessonplans.html#causal-inference",
    "href": "lessonplans.html#causal-inference",
    "title": "Lesson Plans",
    "section": "Causal inference",
    "text": "Causal inference\n\nCorrelation and causation\n\n\nPath analysis"
  },
  {
    "objectID": "lessonplans.html#bayesian-methods",
    "href": "lessonplans.html#bayesian-methods",
    "title": "Lesson Plans",
    "section": "Bayesian methods",
    "text": "Bayesian methods\n\nBayes theorem\n\n\nMCMC estimation"
  },
  {
    "objectID": "lessonplans.html#bayesian-models",
    "href": "lessonplans.html#bayesian-models",
    "title": "Lesson Plans",
    "section": "Bayesian models",
    "text": "Bayesian models\n\nJAGS\n\n\nHierarchical Bayesian models"
  },
  {
    "objectID": "lessonplans.html#advanced-bayesian-models",
    "href": "lessonplans.html#advanced-bayesian-models",
    "title": "Lesson Plans",
    "section": "Advanced Bayesian models",
    "text": "Advanced Bayesian models\n\nCustom models"
  },
  {
    "objectID": "lessonplans.html#mixture-models",
    "href": "lessonplans.html#mixture-models",
    "title": "Lesson Plans",
    "section": "Mixture models",
    "text": "Mixture models\n\nOverdispersion\n\n\nLatent variables"
  },
  {
    "objectID": "lessonplans.html#multivariate-methods",
    "href": "lessonplans.html#multivariate-methods",
    "title": "Lesson Plans",
    "section": "Multivariate methods",
    "text": "Multivariate methods\n\nDimensionality reduction\n\n\nMultivariate models"
  },
  {
    "objectID": "lessonplans.html#cautionary-tales",
    "href": "lessonplans.html#cautionary-tales",
    "title": "Lesson Plans",
    "section": "Cautionary tales",
    "text": "Cautionary tales\n\nInformation theoretic approaches\n\n\nEthics in modeling"
  },
  {
    "objectID": "lessonplans.html#emerging-methods",
    "href": "lessonplans.html#emerging-methods",
    "title": "Lesson Plans",
    "section": "Emerging methods",
    "text": "Emerging methods\n\nEmerging methods"
  },
  {
    "objectID": "intro.html#how-to-become-a-quantitative-ecologist",
    "href": "intro.html#how-to-become-a-quantitative-ecologist",
    "title": "1  Thinking like a Modeler",
    "section": "1.2 How to become a quantitative ecologist",
    "text": "1.2 How to become a quantitative ecologist\nBecoming ‘quanty’ requires a shift in how you think about the world. For some of us, it is a very intuitive way to approach things. For others, it may be a bit dissimilar to how you usually approach things. There are a few general mindset shifts that I think can be helpful as you embark on your journey of becoming better at quantitative reasoning, modeling, and coding.\n1) You are learning a new language The best predictor of being good at coding is not experience with math, computers, or anything techy - it is being good at learning new languages. I think this applies not just to the coding portion of the course, but really everything with thinking like a modeler. What modelers do, routinely, is translate from one language to another. Models can be described verbally. Let’s go on a throwback to high school physics: Newton’s Laws of Motion. Newton’s Second Law says that the acceleration of any object depends on how much mass it has, and how much force is applied to it in a direction. This statement can be translated into an equation as \\(F=ma\\). We could also represent it graphically.\nIt is important to get comfortable moving between these three modes of thinking about models: verbal or conceptual descriptions, equations, and graphical models. It is also okay to only be comfortable with one of those modes, and to get better as time goes on at expressing models in the other methods. For instance, right now you may be most comfortable with a verbal model describing how you think the world works, and unfamiliar or even intimidated by the others. That’s okay! We’ll get there.\nYou could also add a fourth mode (code), but I would actually caution against that. Many people become too reliant on expressing models in code, which makes them irreproducible in the long run and less easy to translate. Code is ephemeral. Package versions change, languages change, and there are\n2) Crappy sketches are lifesavers\n3) Being vague-ish helps\n4) You’re wrong most of the time\n5) Rules are meant to be broken\n6) Question everything\n7) Picture yourself as a fish\nEquations and math - Thinking in terms of equations - Getting comfortable with equations and math - start with the deterministic bit - translate - learn new languages - interactions, separate effects, multiple effects - forget about the math for now\nGraphs and conceptual models - sketching is good - whiteboards are great - cartoons are excellent - arrows are good - a series of scatterplots goes a long way - story board your hypotheses - starting to draw a line when it might not be linear, placing points - Thinking in terms of graphs - drawing a conceptual model first helps - pictionary - rough sketch before fine details - up / down, not point estimates\nSimplification - vagueness is good - patterns over details - simplifying, what’s important - the exceptions don’t matter; focus on the general, that’s what we explain - ignore the small things - don’t get too complex - gestalt - explain it like a toddler - what is irrelevant? - don’t build an airplane - simplify reality - ignore irrelevant details - understanding is better than precision - precision &lt; approximation (not lazy) - #ish\nIncorrectness - get comfortable with being wrong - get used to being wrong - get used to being kinda right too - intuition goes a long way, but be prepared to justify it - assume you’re wrong - don’t make perfect enemy of the good - okay to get like 80% of the way there - start from first principles, okay not to know, rationally work out possibilities\nNatural history knowledge - what’s the question or hypothesis, not what’s the data - answer the question; not just fancy math - sometimes, the math does come first (but it shouldn’t! and should be rare) - what do you think, as an ecologist/biologist, matters? - keep biological reality in mind - scenario / specific context thinking - picture yourself as a fish - anthropomorphize the problem\nBreaking rules - Rules, not important - relaxing assumptions, duck; doesn’t actually say i can’t do that - you may not use, may not touch (assumptions model will follow) - solving same task in different ways; - no real answers - guidelines - What’s important to forget - What’s possible - What’s been done - What hasn’t been done (and why, or why not?) - What people are doing in other disciplines\nQuestion things - under what conditions would we see this? when would we not see this? - ask yourself what if, be your own devils advocate - do reality checks frequently - be explicit about assumptions - can you answer ‘why’ you think something should be that way? - what happens if we fiddle with this part? - reason with abstraction - what would happen if that weren’t true? what if there was no gravity? - question everything / be skeptical / not confident in models - what happens at zero? does the model fail correctly? - don’t put all your eggs in one basket - not knowing how task will be evaluated, think about what could go wrong - consider the possibilities"
  },
  {
    "objectID": "intro.html#hypothesis-framing-and-asking-questions",
    "href": "intro.html#hypothesis-framing-and-asking-questions",
    "title": "2  Thinking like a Modeler",
    "section": "2.4 Hypothesis framing and asking questions",
    "text": "2.4 Hypothesis framing and asking questions\n\nhow the world works\nsort of an if/then\nnot a null hypothesis\nnot a testable assumption\nnot a testable prediction\ndifference between hypotheses and predictions\nask questions you think are interesting, not possible\nbased in observation"
  },
  {
    "objectID": "intro.html#what-is-a-model",
    "href": "intro.html#what-is-a-model",
    "title": "2  Thinking like a Modeler",
    "section": "2.5 What is a model",
    "text": "2.5 What is a model\n\na simplification\nall wrong, all useful\nairplanes\ncore assumptions\nkey characteristics\nparsimony\ncomplexity"
  },
  {
    "objectID": "lessonplans.html#week-1-inference",
    "href": "lessonplans.html#week-1-inference",
    "title": "Lesson Plans",
    "section": "Week 1: Inference",
    "text": "Week 1: Inference\n\nIntroduction to the course\nSchedule\n\nHow to approach this course (15 min)\nIntroductions (20 min)\nHow to think like a modeler (15 min)\nEcology Fishbowl (25 min)\nGoal setting (15 min)\n\nLearning outcomes\nAfter this lecture, students should know:\n\nThe learning approach for this course will be different from most content-based courses and emphasizes thinking and self-teaching over content mastery\nEveryone is starting from a different place and will progress through their quantitative training journey at different paces\nLearning to code in R is like learning a new language\nMistakes are expected during the modeling process\nModeling requires a mindset that embraces ambiguity, abstraction, relationships, and being wrong\nIt is okay to challenge assumptions and question models\nGoals should be SMART: Specific, Measurable, Achievable, Relevant, and Time-bound\n\nWhy are we playing Ecology Fishbowl? a) because it is fun and we are going to be spending a lot of time together this semester, and b) because it encourages creativity, allows for ambiguity, prioritizes the most important features, and gets us all to communicate vague ideas, consider alternative possibilities, and focus on generalities over precision\nReading(s)\n\nThinking Like a Modeler\n\nAssignment(s)\n\nUsing the SMART framework, identify 5-10 goals you have for this course. What do you aim to achieve by Week 15? What progress would you like to make? What would success look like to you at the end of this course? These could be related to coding,modeling, mathematics, comfort with equations, data management, computer literacy, research progress, statistical literacy, self-confidence, etc. Be ambitious, but realistic in what you can achieve. Your self-assessment of progress towards reaching your self-determined goals counts for 10% of your grade in this course because I want you to decide what you want to get out of the course and what will be most useful to you in your academic career. Due January 22\n\n\n\nHypotheses and models\nSchedule\n\nHypothesis framing (20 min), including workshopping hypotheses\nWhat is a model (15 min)\nRectilinear Pictionary (30 min)\nThe history of statistics and eugenics (15 min)\nApproaches to inference (10 min)\n\nLearning outcomes\nAfter this lecture, students should know:\n\nA hypothesis is a statement about how you think the world works\nA prediction is what you would expect to observe if your hypothesis is true\nPredictions let us test hypotheses to make inference about the world\nResearch questions and hypotheses should be grounded in biological reality, not by data or modeling constraints\nKnowing how to code well enables you to ask, and answer, (almost) any question in ecology and evolutionary biology\nModels are simplified representations of the real world\nModels can be represented verbally, graphically, with equations, and in code; modelers should be able to translate between these different formats\nAdding complexity does not always result in a better model, and parsimonious models are favorable to unnecessarily complex ones\nMuch of modern statistics is rooted in eugenics\n\nWhy are we playing Rectilinear Pictionary? Because it forces abstraction, breaks difficult concepts down into simplified components, requires physically drawing a model, lets us embrace ambiguity, and gets us all more comfortable being terribly wrong\nReading(s)\n\nHypotheses and models\nOptional: Hippopotamus\n\nAssignment(s)\n\nA Badly Broken Code Due January 25\nOptional: listen to Dessa while coding\n\n\n\nLab 1: Getting comfortable in R and RStudio\nObjectives\n\nInstall R and RStudio\nGet comfortable with basics of programming in R\nDevelop enough coding literacy to correct a script littered with errors\n\nActivities and materials\n\nDownloading R and RStudio\nRStudio panes\nCalculation\nAssigning objects\nTypes of objects\nFunctions\nInstalling packages\nFinding help\nBasic coding principles"
  },
  {
    "objectID": "lessonplans.html#week-2-random-variables",
    "href": "lessonplans.html#week-2-random-variables",
    "title": "Lesson Plans",
    "section": "Week 2: Random variables",
    "text": "Week 2: Random variables\n\nRandom variables\nSchedule\n\nGuillemot chick survival (5 min) video\nSimulating draws from a Bernoulli distribution (15 min)\nActivity: probability of guessing horse or laminator correctly (30 min)\nBinomial distribution (10 min)\nLaw of large numbers (20 min)\nVectors (10 min)\n\nLearning outcomes\nAfter this lecture, students should know:\n\nRandom variables are described by probability distributions\nRandom variables can be described mathematically\nThe Bernoulli distribution simulates one flip of a coin\nThe probability of success is on a per trial basis\nThe Binomial distribution simulates multiple trials\nIncreasing sample sizes towards infinity results in more stable estimates of probability from observations\nR has built-in functions for simulating most probability distributions\n\nReading(s)\n\nRandom variables\nOptional: Horse or Laminator\n\n\n\nSimulation and probability\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\nSimulating data can help us understand what assumptions we’ve made and lets us build and test models when we know the answer\nFunctions are a set of code that take inputs and return outputs based on a set of instructions that the user gives\nProbability distributions can be discrete or continuous\nProbability distributions have different ranges (sets of possible values) and supports (values they can take)\nDiscrete probability distributions have probability mass, and those probabilities must sum to 1\nContinuous probability distributions have probability density under each potential value and the area under the curve must sum to 1\nEmpirical data can be used to estimate probability of observing data\nProbability density or mass functions can be used to estimate the probability of observing data\nA p-value is the probability of observing your data, or data more extreme, if the null is true\nThe normal (Gaussian) distribution has a range of \\(\\in (-\\infty, \\infty)\\) and support of all real numbers\nThe Bernoulli distribution has a range of \\(\\in (0, 1)\\) and a support of \\((0,1)\\)\nThe probability mass function for the Bernoulli distribution is \\(P(X=1) = p, P(X=0) = 1-p\\)\nReal numbers are those found on the number line (rational and irrational numbers) but not imaginary; integers are whole numbers that can be positive, negative, or zero; rational numbers can be expressed as ratio of two integers; irrational numbers cannot\n\nReading(s)\n\nSimulation and Probability\n\nAssignment(s)\n\nProbability of Chonky Guillemots\n\n\n\nLab 2: Data structures"
  },
  {
    "objectID": "lessonplans.html#week-3-linear-models-i",
    "href": "lessonplans.html#week-3-linear-models-i",
    "title": "Lesson Plans",
    "section": "Week 3: Linear models I",
    "text": "Week 3: Linear models I\n\nComponents of linear models\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nModel fitting\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 3: Functions and for loops"
  },
  {
    "objectID": "lessonplans.html#week-4-linear-models-ii",
    "href": "lessonplans.html#week-4-linear-models-ii",
    "title": "Lesson Plans",
    "section": "Week 4: Linear models II",
    "text": "Week 4: Linear models II\n\nEvaluating models\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nPrediction and inference\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 4: Data visualization"
  },
  {
    "objectID": "lessonplans.html#week-5-linear-models-ii",
    "href": "lessonplans.html#week-5-linear-models-ii",
    "title": "Lesson Plans",
    "section": "Week 5: Linear models II",
    "text": "Week 5: Linear models II\n\nUncertainty\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nProbability distributions\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 5: Exam #1"
  },
  {
    "objectID": "lessonplans.html#week-6-generalized-linear-models",
    "href": "lessonplans.html#week-6-generalized-linear-models",
    "title": "Lesson Plans",
    "section": "Week 6: Generalized linear models",
    "text": "Week 6: Generalized linear models\n\nLink functions\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nDistributions\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 6: Data management"
  },
  {
    "objectID": "lessonplans.html#week-7-mixed-effects-models",
    "href": "lessonplans.html#week-7-mixed-effects-models",
    "title": "Lesson Plans",
    "section": "Week 7: Mixed effects models",
    "text": "Week 7: Mixed effects models\n\nNon-independence\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nRandom effects\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 7: Dimensionality and data"
  },
  {
    "objectID": "lessonplans.html#week-8-causal-inference",
    "href": "lessonplans.html#week-8-causal-inference",
    "title": "Lesson Plans",
    "section": "Week 8: Causal inference",
    "text": "Week 8: Causal inference\n\nCorrelation and causation\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nPath analysis\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 8: Reporting standards"
  },
  {
    "objectID": "lessonplans.html#week-9-bayesian-methods",
    "href": "lessonplans.html#week-9-bayesian-methods",
    "title": "Lesson Plans",
    "section": "Week 9: Bayesian methods",
    "text": "Week 9: Bayesian methods\n\nBayes theorem\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nMCMC estimation\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 9: Handling missing data"
  },
  {
    "objectID": "lessonplans.html#week-10-bayesian-models",
    "href": "lessonplans.html#week-10-bayesian-models",
    "title": "Lesson Plans",
    "section": "Week 10: Bayesian models",
    "text": "Week 10: Bayesian models\n\nJAGS\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nHierarchical Bayesian models\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 10: Reproducible workflows"
  },
  {
    "objectID": "lessonplans.html#week-11-advanced-bayesian-models",
    "href": "lessonplans.html#week-11-advanced-bayesian-models",
    "title": "Lesson Plans",
    "section": "Week 11: Advanced Bayesian models",
    "text": "Week 11: Advanced Bayesian models\n\nCustom models\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 11: Exam #2"
  },
  {
    "objectID": "lessonplans.html#week-12-mixture-models",
    "href": "lessonplans.html#week-12-mixture-models",
    "title": "Lesson Plans",
    "section": "Week 12: Mixture models",
    "text": "Week 12: Mixture models\n\nOverdispersion\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLatent variables\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 12: Archiving data and code"
  },
  {
    "objectID": "lessonplans.html#week-13-multivariate-methods",
    "href": "lessonplans.html#week-13-multivariate-methods",
    "title": "Lesson Plans",
    "section": "Week 13: Multivariate methods",
    "text": "Week 13: Multivariate methods\n\nDimensionality reduction\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nMultivariate models\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 13: Independent projects"
  },
  {
    "objectID": "lessonplans.html#week-14-cautionary-tales",
    "href": "lessonplans.html#week-14-cautionary-tales",
    "title": "Lesson Plans",
    "section": "Week 14: Cautionary tales",
    "text": "Week 14: Cautionary tales\n\nInformation theoretic approaches\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nEthics in modeling\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)\n\n\n\n\n\nLab 14: Independent projects"
  },
  {
    "objectID": "lessonplans.html#week-15-emerging-methods",
    "href": "lessonplans.html#week-15-emerging-methods",
    "title": "Lesson Plans",
    "section": "Week 15: Emerging methods",
    "text": "Week 15: Emerging methods\n\nEmerging methods\nSchedule\n\n\n\nLearning outcomes\nAfter this lecture, students should know:\n\n\n\nReading(s)\n\n\n\nAssignment(s)\n\n\n\nEvaluation(s)"
  },
  {
    "objectID": "intro.html#how-to-approach-this-course-1",
    "href": "intro.html#how-to-approach-this-course-1",
    "title": "1  Thinking like a Modeler",
    "section": "1.3 How to approach this course",
    "text": "1.3 How to approach this course\nThe goal of this course is to get you to think quantitatively i.e. recognizing that data are generated by deterministic and stochastic processes, both of which are defined by you (or your understanding of the biology of the system that you are modeling or making inference on). No one can really tell you how to model or analyze your data - there is no lookup table for ecological modeling.\nAfter this course, you should know enough to 1) be dangerous on Google / StackOverflow to find analytical / modeling approaches that match your questions and data, and 2) write or adapt code to implement those models and not be reliant on a GUI. The goal is not complete mastery of statistics, modeling, or data analysis.\n\n\n\nThe goal of this course is to get you closer to becoming dangerous."
  },
  {
    "objectID": "intro.html#the-end-goal",
    "href": "intro.html#the-end-goal",
    "title": "1  Thinking like a Modeler",
    "section": "1.3 The end goal",
    "text": "1.3 The end goal\nThe goal of this course is to get you to think quantitatively i.e. recognizing that data are generated by deterministic and stochastic processes, both of which are defined by you (or your understanding of the biology of the system that you are modeling or making inference on). No one can really tell you how to model or analyze your data - there is no lookup table for ecological modeling.\nAfter this course, you should know enough to 1) be dangerous on Google / StackOverflow to find analytical / modeling approaches that match your questions and data, and 2) write or adapt code to implement those models and not be reliant on a GUI. The goal is not complete mastery of statistics, modeling, or data analysis.\n\n\n\nThe goal of this course is to get you closer to becoming dangerous."
  },
  {
    "objectID": "intro.html#shifting-how-you-think",
    "href": "intro.html#shifting-how-you-think",
    "title": "1  Thinking like a Modeler",
    "section": "1.2 Shifting how you think",
    "text": "1.2 Shifting how you think\nBecoming ‘quanty’ requires a shift in how you think about the world. For some of us, it is a very intuitive way to approach things. For others, it may be a bit dissimilar to how you usually approach things. There are a few general mindset shifts that I think can be helpful as you embark on your journey of becoming better at quantitative reasoning, modeling, and coding.\n1) You are learning a new language The best predictor of being good at coding is not experience with math, computers, or anything techy - it is being good at learning new languages. I think this applies not just to the coding portion of the course, but really everything with thinking like a modeler. What modelers do, routinely, is translate from one language to another. Models can be described verbally. Let’s go on a throwback to high school physics: Newton’s Laws of Motion. Newton’s Second Law says that the acceleration of any object depends on how much mass it has, and how much force is applied to it in a direction. This statement can be translated into an equation as \\(F=ma\\). We could also represent it graphically.\nIt is important to get comfortable moving between these three modes of thinking about models: verbal or conceptual descriptions, equations, and graphical models. It is also okay to only be comfortable with one of those modes, and to get better as time goes on at expressing models in the other methods. For instance, right now you may be most comfortable with a verbal model describing how you think the world works, and unfamiliar or even intimidated by the others. That’s okay! We’ll get there.\nYou could also add a fourth mode (code), but I would actually caution against that. Many people become too reliant on expressing models in code, which makes them irreproducible in the long run and less easy to translate. Code is ephemeral. Package versions change, languages change, and there are\n2) Crappy sketches are lifesavers\nOne of the best ways to change how you think about models and put yourself in a quantitative mindset is to sketch out some rough ideas. They don’t need to be pretty, they just need to convey what you’re thinking. Whiteboards, a legal pad, a pile of scratch paper, a notebook, or the back of an envelope are all helpful things to keep handy for explaining a model.\nA series of scatterplots can go a long, long way in terms of walking through your thought process and forces you to carefully explain the relationship that you predict to exist between two or more variables, which can help you understand your own thinking about how the world works better. We will talk about this (how the world works) more in the next lecture on hypothesis framing. For now, let’s just think about representing our ideas in terms of sketches. Think of these sketches as a storyboard of your hypotheses. Each individual component tells you a little bit more about the overall model even if each is an incomplete bit.\nYou can tell what a sketch is meant to represent even from pretty incomplete drawings. It is kind of like Pictionary. Similarly, you can tell what a model is even from a really rough representation. Arrows in conceptual models can be especially useful to demonstrate how you are thinking about the causal relationship between variables or what you think the true underlying ecological process is, even if you are not adopting a causal modeling framework.\nHow to start a sketch: 1) Draw an x-axis and a y-axis. 2) Think about what your response variable is - what are you interested in knowing about and how it changes? Label that as your y-axis. 3) What do you think causes changes in your response variable? Label that on your x-axis. 4) Ask yourself, at low values of X, what do I expect to be the relative level of Y? Place a point there. 5) Ask yourself, at high values of X, what do I expect to be the relative level of Y? These don’t need to be precise estimates - just higher, lower, a lot higher, etc. Place a point there. 6) Continue this process for a few intermediate points, especially if you think there may be non-linear relationships. Keep track of any thoughts you have like ’well, it depends on Z…” because that might suggest you’ve got some moderating variables and you might need to introduce a second line, perhaps with a different color. 7) Step back and look at your graph. Does it pass the gut check? If not, maybe revise it. 8) Criticize your graph and question the key assumptions. Under what scenarios does it break down? Are there situations you aren’t considering? When is this relationship not true? Is it generally true though? Are there alternatives you aren’t considering? 9) Add lines or colors to your graph representing hypothesized relationships under different conditions, with different sets of moderator variables, etc. 10) If there are alternative explanations, sketch them out as well. Keep track of which hypotheses go with which graph in your mind (i.e., what is your explanation for why you might see the pattern you predict to observe).\n3) Being vague-ish helps\nGeneral patterns are far, far more important than details. What we are interested in from a modeling standpoint is the patterns we can explain. We are rarely interested in the exception to the rule, because it is extremely difficult to model or predict the exceptions. Instead, we can model the rule, and then those exceptions become interesting challenges either to help us better understand the assumptions we have baked into our model, or to discover some truly interesting ecological phenomena we don’t yet understand. In other words, the exceptions and outliers are interesting and let us ask why, but, when we are developing a model, we should aim to fit the general pattern and not encompass absolutely everything.\nIgnore the small things. Simplifying, sometimes to the extreme of oversimplification, is the best place to start with a model. What are the core things that are most important to include? What is irrelevant or can be ignored, at least for now, to be able to focus on the big picture changes? If you can explain your model to a toddler, you’re on the right track. I like to keep the Up Goer Five in mind as a good model of how a rocket ship works, but in toddler terms. You can even try testing out how you describe your model to make sure it is simple enough with the Simple Writer.\n4) You’re wrong most of the time\nWhen you’re modeling, intuition goes a long way, but be prepared to justify it. You can of course start from first principles and work your way up to what you suppose to be true, but you can also start from what you suppose to be true and then work backwards from there to figure out your own underlying assumptions. You can then also work from those core assumptions to see if there are other possibilities.\nYou should also be comfortable being good enough. Not great, not perfect, but good enough. Don’t let the perfect be the enemy of the good. If you get like 80% of the way there with a model, either in terms of how much it explains, how satisfied you are with it, or any other metric, that’s pretty good! Much better than 0% or even 10% of the way there. Nothing has to get to 100% because let’s face it, you’re always going to be wrong in some ways and that’s perfectly okay.\n5) Rules are meant to be broken\nThere are a lot of ‘rules’ to follow in statistics, but (and again, stealing this from Quantitude!) similar to the pirate’s code, “they’re more like guidelines” anyway. Just because something has always been done one way, or is done commonly in the literature, doesn’t mean it is the right way to do something. Just because something hasn’t been done also doesn’t mean that it can’t or shouldn’t be done. So hang the rules, and do what makes biological sense.\n6) Question everything\nWe make a lot of assumptions in modeling. It is very important to question those assumptions. Once you’ve expressed a model (verbally, graphically, mathematically, in code, or elsewise), ask yourself under what conditions would we expect this model to hold up? When would we not see this pattern? What happens at zero? Under what conditions does your model fail? If it fails, does it fail correctly? Does it fail when it should?\nBe your own devils advocate and question your model structure, the data generating process, and the assumptions baked into your model. Can you answer why you think something should behave in the way you expect? What would happen if one of your assumptions were not true? What if there was no gravity? If we changed one part, what would happen?\nAlso remember that there could be nearly infinite different models, and you’re likely only considering a few of them. Keep all the possible models in mind, and consider if any of them are better representations of what you’re trying to represent. Don’t put all your eggs in one basket and fully convince yourself that you’ve developed the best model, because let’s face it, your model is definitely wrong, and maybe there are better models. Be okay with being wrong, and interrogate yourself and your model as you do frequent reality checks.\n7) Picture yourself as a fish\nThe single most important “rule” (keeping in mind we just tossed rules out the window) in ecological modeling is that your models must be biologically relevant. Start with the natural history of the system. What do you know about the species, populations, or communities you are modeling? What can and cannot be true in those systems? Don’t let the models lead your thinking, force the models to help you answer the questions you want to answer. Similarly, start with the biological question you want to answer or hypothesis you want to address, not what the data are or what they will allow you to do.\nI say ‘picture yourself as a fish’ because I think it can really help sometimes to anthropomorphize the problem. If you’re trying to figure out how a model should work, draw what you expect or predict to observe, and think about your underlying hypotheses, it can sometimes help to insert yourself into the scenario. Keep the ecological and evolutionary context in mind."
  },
  {
    "objectID": "lab1.html#downloading-r-and-rstudio",
    "href": "lab1.html#downloading-r-and-rstudio",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Downloading R and RStudio",
    "text": "Downloading R and RStudio\nThroughout this course, we will be almost exclusively using R to read and manipulate data, simulate data, fit models, etc. R is a programming language, so we will not access it directly. Instead, we will use RStudio, which is a Graphical User Interface (GUI) for working with R. R also has its own GUI, but it is not nearly as functional as RStudio, which is how the vast majority of R users interact with R. It is worth knowing that the R GUI exists, however, because sometimes people will mistakenly open it, do a lot of work, and then lose that work. The reason for that is because the R GUI is only the console, which we will get to in a minute. First, we will install both R and RStudio.\n\nFirst, install R. Follow the instructions for your operating system here: https://cran.r-project.org/\nInstall RStudio. Note that this needs to be done after installing R. Follow the instructions for your operating system here: https://posit.co/download/rstudio-desktop/\n\nOnce you have both R and RStudio installed, make sure you know how to open RStudio. Depending on your operating system and how you’ve set up your computer, it may add a shortcut icon to access the R GUI. Do not use it. It will pull up an older looking GUI which is only the console (which we are still getting to soon!). Always use RStudio to interact with R (…unless you’re running shell scripts or some other niche purposes, but if you’re doing that, you probably aren’t reading instructions on how to download and access R).\n\n\n\nThe logo on the right is RStudio, which is the one you want."
  },
  {
    "objectID": "lab1.html#rstudio-panes",
    "href": "lab1.html#rstudio-panes",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "RStudio panes",
    "text": "RStudio panes\nWhen you open RStudio for the first time, the upper left is the Source pane. This is where you will open scripts, edit code, run analyses, type notes, etc. To use a cooking analogy, the Source pane is your recipe book. You can always come back to it, make little changes, or leave notes about what to fix for next time. But it is not the actual meal, just instructions for making a meal.\nIn the bottom left is the Console. The Console is the process of cooking. It can be done following the recipe by running chunks of code from the Source, or you can wing it. If you decide to ‘cook’ on the fly by running code directly from the Console, you will save no notes on what on earth you put into the meal, you cannot go back and fix things easily, and you cannot share your recipe with anyone else. You also might not remember the order in which you did things. So if what you did worked and your ‘meal’ comes out great, you will not know what you did to make that happen so it is not reproducible. The only real use for the console is for code that you only want to run once, e.g. foundational things like installing packages; to really overcook this analogy (ha), you only install a kitchen counter once, not at the start of every recipe. More on packages later.\nIn the upper right is the Environment (along with a few other tabs that you’ll use far less). This is the current version of your meal, i.e. if you’re making a stew, this is everything that is currently in the pot or mise en place and ready to go. It is objects (think of these as ingredients) that you have created with your cooking and are now available to either eat or combine with other objects (i.e., ingredients).\nIn the bottom right are your Files, Plots, and Help - all of which you will use frequently (and other tabs you will use far less often). Think of these as things that are available to you, but external to your current kitchen counter and stew. The Files tab will let you browse files on your computer; think of it is your kitchen where you can get more resources for your meal if you are in your current working directory (more on directories later), or your house if you browse for other files on your computer. Your Plots tab is the equivalent of taking a picture of the current version of your meal, either to be able to visualize what you’ve currently got, or to share with others. The Help tab is exactly what it sounds like - the person you call to ask how to use the stand mixer (i.e. the function) and what to put into it (i.e., the arguments passed to the function) to make the perfect waffle. It’s up to you to know that you want to use the stand mixer to make waffles in the first place.\n\nSome terms and definitions\nSince that analogy is now burnt to a crisp, let’s unpack some of the extra terms in there and what they are because you will use them frequently in this course.\n\nPackages\nPackages are a collection of functions designed to work together to accomplish some specific outcome. Many packages are hosted on CRAN, however, you can also find R packages on repositories like GitHub. One way to find packages is through CRAN Task Views (e.g. these are all packages associated with meta-analysis https://cran.r-project.org/web/views/MetaAnalysis.html), but more often you’ll just Google what you want to do and find a package that way.\n\n\nObjects\nObjects are what are in your environment. They can take lots of different forms, have different classes, etc. Most objects are created by using the assignment operator &lt;- to pass the output of some code to a named object. We will talk about different types of objects more during the semester as we encounter them.\n\n\nWorking directory\nYour Files tab lets you see two things: files, and directories. Directories are the organizational structure for how you store data on your computer; you can think of them as folders for the most part, though folders are GUI ways to visualize directories and directories have a clear nested structure.\nYou working directory is very important when coding. File paths are relative to your current working directory, so when you read files in you must know both what your current working directory is; most of the time if you get an error reading in a file, it is because the path to the file is incorrect. In the cooking analogy, you are cooking on the kitchen countertop, which is nested within the kitchen, which is nested within your house. You could move directories within the kitchen, such as moving to the sink, or you could move up several levels to go to the living room. If you try to call a file that is not in your current working directory, you will get an error. For example, if you are in the living room and tell R to pick up your cutting board, it will say it does not exist.\nRelative file paths are extremely useful in coding. Relative file paths begin in your current working directory. To load a file from your current directory, begin the file path with ./. To load a file from the working directory above you (i.e. if you’re working at the kitchen counter, the kitchen is the next hierarchical level above you), use ../. One way to remember the difference between one dot and two is that if the dots represent your feet, one shows where you are standing - but two means you’ve hopped somewhere new. Since directories are nested within each other, you can also combine these into longer relative paths. So, for example, if I am in the living room but I want R to go into the kitchen, then into the cabinet, and take out a cutting board, the path would be something like: ../kitchen/cabinet/cutting_board.txt.\nSetting your working directory. There are two ways to set your Working Directory. One is the click-and-point way, where you can go to Session &gt; Set Working Directory &gt; Choose Location which will open up your normal file browser application and you can navigate around to find where you want to set as your current working directory. This is easier when you’re starting out and getting used to directory structure and how you have your files organized, but is not reproducible so can cause headaches if you think you’re in a different directory than you are later on when running a script. A better option is to use the function setwd() with an absolute path to a directory. An absolute path is one that starts at your home directory (e.g. on Linux or Mac, ~/, on Windows typically something like C:/). For example, I might run something like setwd(\"~/Desktop/BIOL431\") at the start of my script, and then use relative paths throughout once I am in the directory where I have my files for analysis and where I want to save my output.\n\n\nFunctions\nFunctions take input as arguments, and return output. To see what arguments can be passed to a function, and also what its output will be, you can use the Help tab to search for a function. Or, much more quickly, use a ? followed by the function name if it is in a package that is currently loaded (e.g. ?rbinom). If it is in a package that is not currently loaded, use ?? instead (e.g. ??glmer).\n\n\n\nCustomizing RStudio\nThere are three modifications many people will want to make to RStudio:\n\nTo change the theme, go to Tools &gt; Global Options. For example, you may prefer a dark theme if you’re coding frequently.\nRearrange the panes. There is a window-like icon to the left of ‘Addins’ in the tool bar; select the drop down and you can customize which panes are in which corners. For example, I prefer to move my Console to the right so I can see my code in parallel and make the Environment tab really small because I do not need to check it frequently.\nYou can change the font and font size, which can be helpful depending on your screen size and eyesight."
  },
  {
    "objectID": "lab1.html#calculation",
    "href": "lab1.html#calculation",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Calculation",
    "text": "Calculation\n\n42/3\n\n[1] 14\n\n1.2*2\n\n[1] 2.4\n\n4 + 7\n\n[1] 11\n\n1 - 0.2\n\n[1] 0.8\n\n(11-2)/3\n\n[1] 3"
  },
  {
    "objectID": "lab1.html#assigning-objects",
    "href": "lab1.html#assigning-objects",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Assigning objects",
    "text": "Assigning objects\n\nx &lt;- 42\ny &lt;- 3\nx/y\n\n[1] 14"
  },
  {
    "objectID": "lab1.html#types-of-objects",
    "href": "lab1.html#types-of-objects",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Types of objects",
    "text": "Types of objects\n\nclass(\"words\")\n\n[1] \"character\"\n\nclass(4.2)\n\n[1] \"numeric\"\n\nclass(1:10)\n\n[1] \"integer\"\n\nclass(TRUE)\n\n[1] \"logical\"\n\nclass(x)\n\n[1] \"numeric\"\n\nclass(factor(letters[1:5]))\n\n[1] \"factor\"\n\na &lt;- 1:10\nb &lt;- 11:20\n\ndat &lt;- data.frame(a, b)\n\nas.numeric(\"two\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nsqrt(-1)\n\nWarning in sqrt(-1): NaNs produced\n\n\n[1] NaN"
  },
  {
    "objectID": "lab1.html#functions-1",
    "href": "lab1.html#functions-1",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Functions",
    "text": "Functions\n\nseq(1, 10, by=0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\nseq(1, 10, l=30)\n\n [1]  1.000000  1.310345  1.620690  1.931034  2.241379  2.551724  2.862069\n [8]  3.172414  3.482759  3.793103  4.103448  4.413793  4.724138  5.034483\n[15]  5.344828  5.655172  5.965517  6.275862  6.586207  6.896552  7.206897\n[22]  7.517241  7.827586  8.137931  8.448276  8.758621  9.068966  9.379310\n[29]  9.689655 10.000000"
  },
  {
    "objectID": "lab1.html#installing-packages",
    "href": "lab1.html#installing-packages",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Installing packages",
    "text": "Installing packages\nTo install a package from CRAN, you can use the install.packages() function with the name of the package inside the open parentheses in quotes (e.g. install.packages(\"lme4\")) to install the lme4 package which is useful for linear models. To install packages from other sources, follow the package developer’s instructions.\n\n# install.packages(\"lme4\")\n# install.packages(\"remotes\")\n# remotes::install_github(\"/MetBrewer\")"
  },
  {
    "objectID": "lab1.html#finding-help",
    "href": "lab1.html#finding-help",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Finding help",
    "text": "Finding help\n\n?seq\n??plot"
  },
  {
    "objectID": "lab1.html#basic-coding-principles",
    "href": "lab1.html#basic-coding-principles",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Basic coding principles",
    "text": "Basic coding principles\n\nVariable names\nIt is important to choose good variable names when coding. This is one of the many, many times where you should do as I say, not as I do, to quote my mom. I have terrible variable naming conventions. Please do better than me.\nPick something informative. If your object is a vector of temperatures for the month of January, probably don’t call it var1 for example. Call it something like Jtemp or JanT or something you’ll remember what it is.\nWhen selecting a variable name, go for consistency. Choose a formatting and naming convention that makes sense to you so that you don’t have to repeatedly look at your environment to see what you named something. You’ll also be able to code more quickly if you know what your variable name most likely is rather than having to look it up.\nPick something easy to type. In general, it is good to avoid capitalization, especially when you use a combination of capitalization and not. Why? Because then we forget which letter(s) are capitalized. In the January temperature example, JanT is actually a pretty bad name because we have to remember we capitalized the J and the T. A better name might be something like jan if we’ve got a lot of months of temperature data, or if everything is from the same month, then just temp.\nFor similar reasons, I think it’s a good idea to mostly omit punctuation from a variable name. Some people will do stuff like jan_temp or jan.temp but then I at least can’t remember if I used an underscore or a period. If you go the punctuation route, be consistent about what you use.\nYou can use numbers in variable names, but only if they are not the start of the object name. 2b is not a valid name and R will yell at you for it, but not2b is valid. So if the question is if 2b or not2b is an allowed object name in R, the answer is not2b.\nLastly, do not use function names as object names. The absolute worst thing you could name an object that contains a plot, for example, is plot. Why? Because that’s the function that you use to generate a plot, i.e. plot() and you’ve just overwritten that very basic function with an object name.\n\n\nOverwriting objects\nOnce you create an object, it is typically best to leave it alone. You might overwrite the object for a few lines of code as you’re getting it set up, but don’t overwrite the object later in the script. You will sometimes forget you did that and then think you’re working with a different object that has a different format or structure and it can lead to a lot of confusion especially if you run lines of your script out of order.\n\n\nAnnotation\nDo your future self a favor and annotate your code. You can use the pound symbol # at the start of a line to add a comment or note. You can also add it at the end of a line. Anything following # will not be evaluated (i.e., run) by R.\nYou should take notes on what you tried, what worked, what didn’t work, where you looked for information, why you ran things the way you did, what a particular line of code or function does, etc. That way, when you set down a script and come back to it six months from now, you don’t have to re-learn everything, because past you left helpful notes.\n\n\nCleanliness\nTry to keep your coding environment nice and neat. What I mean by this is to tidy up now and then, remove objects you aren’t currently using, don’t display plots that aren’t part of your current project, and also to have nice, neat, readable scripts.\nI will often use rm(list=ls) at the start of a coding session to remove all objects from my environment. I also like to run dev.off() every now and then to kill the plotting window.\n\nrm(list=ls())\ndev.off()\n\nnull device \n          1 \n\n\nFor coding, try to keep your lines of code fairly narrow and not let them extend so far right that you have to scroll to read them. A good rule of thumb is to keep your lines of code under 80 lines. You can even set a little margin for yourself in RStudio as a visual reminder of this. Go to Tools &gt; Global Options &gt; Code &gt; Display and then tick the box to show a margin and how many characters you want it to be. This helps with reading code quickly.\nIf you feel like your code is getting messy, RStudio has a built-in feature to clean it up. The keyboard shortcut Ctrl + Shift + I will clean up the formatting and spacing for messy code, though only within reason.\nIt can be really useful to structure your code as an outline, because then you can quickly navigate through it. To insert a new section, start a line with a # and then end it with five or more ----- and it will be a section header. To add a subsection within that, use two ## in a row, and a subsubsection is ###, and so on. You can navigate through these sections by clicking the ‘outline’ tab in the upper right corner of your script pane.\n\n\nKeyboard shortcuts\nThere are a lot of keyboard shortcuts in RStudio, which you can find under Tools &gt; Keyboard Shortcuts Help. The ones I use most frequently are Ctrl + A to select all, Crtl + Alt + B to run all lines of code above my current position, Ctrl + Shift + N for a new script, and Ctrl + L to clear my console.\n\n\nDo this not that\nInstead of hardcoding, always aim for softcoding. What I mean by that is to assign values you intend to use to an object and reference that object, rather than manually plugging in numbers. It will make your life so much easier and make it way faster to reuse code in the future if you softcode things. As an example of this, let’s simulate a bit of data. We can force R to use the same starting point for a random number generator (essentially) with set.seed().\n\n# hard coding\nset.seed(42) # the answer to life, the universe, and everythign\nrnorm(n = 1)\n\n[1] 1.370958\n\nrpois(n = 1, lambda = 3)\n\n[1] 2\n\n1.3709584 * 2\n\n[1] 2.741917\n\n# soft coding\nset.seed(42)\nx &lt;- rnorm(n = 1)\ny &lt;- rpois(n = 1, lambda = 3)\nx*y\n\n[1] 2.741917\n\n\nFor similar reasons, try to always use relative, not absolute, paths when referring to files on your computer. This helps us maintain good working directory structures which are neat and tidy, and not a chaotic mess (…again, do as I say, not as I do, and don’t judge my poor file organization on my computer)."
  },
  {
    "objectID": "lab1.html#base-r-and-the-tidyverse",
    "href": "lab1.html#base-r-and-the-tidyverse",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Base R and the tidyverse",
    "text": "Base R and the tidyverse\nThere are two main coding camps when it comes to R: base R, and the tidyverse. Typically, people end up coding in one of these two styles, and the style you choose is what you stick with. In some ways, people stick with the way they first learn, but also I’m convinced some people’s brains are just more suited to one or the other. I code almost exclusively in base R because I like to see every single step laid out and I need to be able to have the concrete changes visualized in front of me so I can see what happens when I change something. In my much belabored cooking analogy, I am tasting that soup each and every single time I add something to it because I cannot process more than one thing at a time and I can’t picture what would happen if I added salt and pepper at the same time. I want to add the salt first, taste the soup, then decide how much pepper to add. Tidyverse people, on the other hand, tend to think in pipes which means they pass the output of one operation right on to the next one sort of like an assembly line. If they are making soup, they say take this soup, add some salt, add some pepper, then give me that to taste. I’m not quite as much of a hater as the guy who wrote this blog post about why students should not start with tidyverse but I do tend to think people who code in base R have a better understanding of what they are doing, can troubleshoot things more quickly and more easily, and have more potential to become much more adept at programming and teaching themselves new skills."
  },
  {
    "objectID": "lab1.html#assignment",
    "href": "lab1.html#assignment",
    "title": "Lab 1: Getting comfortable in R and RStudio",
    "section": "Assignment",
    "text": "Assignment\nAssignment 1: A Badly Broken Code\nOptional: listen to Dessa while coding"
  },
  {
    "objectID": "hypotheses.html#hypothesis-framing-and-asking-questions",
    "href": "hypotheses.html#hypothesis-framing-and-asking-questions",
    "title": "2  Hypotheses and Models",
    "section": "2.1 Hypothesis framing and asking questions",
    "text": "2.1 Hypothesis framing and asking questions\nA hypothesis is a statement about the way you think the world works. Often, we learn in intro classes about research methods that a hypothesis should be phrased as an if/then statement. i.e., if this is true, then I expect to see this. In many ways this is true, but it is also an oversimplification and can easily lead people into a trap of making predictions rather than hypotheses. Predictions are what you would expect to observe if your hypothesis were true. For instance, if I were to hold a marker in my hand and let go of it, I think that it would fall to the floor. My hypothesis is not that it would fall to the floor, however, that’s my prediction. My hypothesis must be a statement about the way the universe works. In the case of the falling marker, my hypothesis is likely that there is some mysterious force which draws my marker downwards (i.e., gravity). There are alternative hypotheses that we could consider in this scenario, which could also explain why my marker falls to the ground when I let go of it. Perhaps the floor is magnetic and my marker is metal and drawn the nearest magnet. Perhaps the marker has an engine in one end which propels it in that direction and I happened to release it in just the right way that it was propelled into the floor. Maybe there is a marker monster that lives under the ground and sucks all markers towards it. In all four cases, I would probably predict to observe the same outcome, but would have a different explanation for it. That’s what the hypothesis really is: the explanation for why you think you would observe your predicted outcome. One hypothesis may also have multiple predictions stemming from it.\nThe way we test our hypotheses is by making predictions that logically follow. If our hypothesis is true, we would predict to observe some outcome. If we then go and collect data and it matches our prediction, we can’t falsify our hypothesis. It doesn’t mean we accept our hypothesis, but we don’t reject it. This notion of falsifiability comes out of a long history of scientific thinking and is what guides a lot of modern statistical thinking. We cannot just ‘accept’ our hypothesis because there are alternative hypotheses that could result in the same predictions. Gravity and the marker-loving-monster dwelling below the surface would both lead to my marker falling. I cannot rule either of those hypotheses out based on predicting that my marker will fall and then observing that it fell. I could still consider those alternative hypotheses as both being likely explanations for what I observed. I would need to go spelunking with market-loving-monster survey gear to test predictions stemming from my alternative hypothesis, which likely is not a testable hypothesis.\nWhenever you come up with a hypothesis, you should consider all the alternatives. This applies not just to the hypothesis itself, but also predictions. What else do you expect to observe if your hypothesis is true?\nNote that a scientific hypothesis is very, very different from how a hypothesis is typically treated in statistics. In statistical hypothesis testing, we are often setting up a ‘null’ hypothesis and an alternative hypothesis, but these are not really biological hypotheses. Thankfully, in this course we don’t do any of that nonsense (…for the most part - we do talk about null hypotheses when we’re getting used to probability density and p values). In modeling, we are much more interested in making predictions that stem from our hypotheses and then modeling our data to see how well it matches those predictions. So our hypotheses truly are scientific hypotheses the vast majority of the time.\nOne of the things that I want to really emphasize in this course is that you should ask the questions, and pose the hypotheses, that you really want to ask and address. Don’t constrain yourself by the statistics, models, or data structures you are familiar with.\n\n2.1.1 In-class exercise: hypotheses and predictions\nGenerate a hypothesis, predictions stemming from it, represent it as a model, and translate that model between a hypothesis, graph, and equations."
  },
  {
    "objectID": "hypotheses.html#what-is-a-model",
    "href": "hypotheses.html#what-is-a-model",
    "title": "2  Hypotheses and Models",
    "section": "2.2 What is a model",
    "text": "2.2 What is a model\nA model is a simplification of the real world. Because our hypotheses are statements about how the world works, we can represent them with models. My absolute favorite podcast (which I highly recommend students in this class check out!) is Quantitude, which is basically like Car Talk but for quantitative methods. The hosts have a great analogy for models, which is model airplanes. When you’re building a model airplane, it should resemble an airplane, you should be able to tell it is an airplane, and it should have some working parts. But if your model of an airplane is so good that when you finish building it you then fly a load of passengers to France, you haven’t built a model airplane, you’ve just built an entire airplane. So you should think of your models as simplifications of the real world, not accurate representations.\nThere’s a classic quote by George Box (who, incidentally, was married to RA Fisher’s daughter Joan) in which he says that “All models are wrong, but some are useful.” The first part is especially true because we’ve simplified reality and cannot capture all the complexities of the real world in a model (if we did, remember that we built an airplane, not a model of an airplane) so they must be incorrect in some ways. It is also useful to remember that all our models are also right, in many ways.\nModels come with a set of assumptions that you make, which are then baked into the model. You should be explicit about what these assumptions are, because they will change how your model behaves.\nModels should be parsimonious, meaning they should be as simple as possible while still explaining the phenomenon you are interested in or your explanation for how the world works. They should be no more complex than necessary. To truly represent how the world works, you wouldn’t make a perfect model of the entire world - that already exists. Only include as much complexity in your model as you need to.\n\n2.2.1 Rectilinear pictionary\nWhy are we playing Rectilinear Pictionary? Because it forces abstraction, breaks difficult concepts down into simplified components, requires physically drawing a model, lets us embrace ambiguity, and gets us all more comfortable being terribly wrong"
  },
  {
    "objectID": "hypotheses.html#a-brief-history-of-statistics-and-eugenics",
    "href": "hypotheses.html#a-brief-history-of-statistics-and-eugenics",
    "title": "2  Hypotheses and Models",
    "section": "2.3 A brief history of statistics and eugenics",
    "text": "2.3 A brief history of statistics and eugenics\nThere are many figures in ecology and evolutionary biology that were giants in their field and helped shape modern science, and who were also extremely problematic individuals (and not just because ‘it was a different time’). Often, the way we learn about these individuals is first through their contributions to the field, and secondarily that they were a ‘bad person’ but that we should still value their intellectual contributions and somehow consider those to be separate from the person. For example, John James Audubon made numerous contributions to ornithology, and also bought and sold enslaved people. You could argue that those are separate aspects of the same person (…you could also argue that he greatly benefited from his status in society as a result of oppressing and enslaving other people and that enabled him to contribute to ornithology, but that is a conversation for a different course…). This is not the case with statistics. Most of modern statistics is built on work done by Sir Francis Galton, Karl Pearson, and Ronald Fisher, all of whom were staunch advocates of eugenics. They also collectively developed the ideas of standard deviation, correlation, regression to the mean, the correlation coefficient, method of moments, \\(\\chi^2\\) test, p-values, principle components analysis, and many other fundamental ideas in statistics. The point is not that these men made exceptional contributions to statistics and also happened to be eugenicists, but rather that they developed statistics to support their eugenicist viewpoints. After all, who could argue with the data they showed to support their view that other races were inferior to white Europeans?\n\n\n\nKarl Pearson (left) and Sir Francis Galton (right), circa 1910\n\n\nMany of the concepts we still use to this day in statistics come out of this tradition. For example, even the idea that a population (in the statistical sense) can be described from a single distribution is rooted in eugenics. At the same time Pearson was writing about mixtures of ‘homogeneous groups’ from a mathematical perspective, he was also advocating for colonialism and destruction of what he considered to be ‘inferior races’ because a more ‘homogeneous’ society was better.\n\n\n\nPearson in Contributions to the Mathematical Theory of Evolution in 1894\n\n\n\n\n\nPearson in National Life from the Standpoint of Science\n\n\nThe specific values, thresholds, cutoffs, and conventions that underlie most of null hypothesis significance testing were developed to support a eugenicist agenda. Always remember that they are not mathematically justified or ordained by some law of nature, but rather prescribed by a handful of like-minded eugenicists over a century ago. You should not feel bad about “breaking” their rules from time to time. Also, statistics is constantly evolving, so there are no hard and fast rules to follow."
  },
  {
    "objectID": "hypotheses.html#goal-setting",
    "href": "hypotheses.html#goal-setting",
    "title": "2  Hypotheses and Models",
    "section": "2.4 Goal setting",
    "text": "2.4 Goal setting\nIn this course, 10% of your grade is based on a self-assessment of progress towards reaching your own goals. People come into this course from many different backgrounds and with different levels of coding and mathematical proficiency. As such, we shouldn’t all be evaluated in the same way, and meeting our own individual goals is probably the most important thing we can do.\nThe goal-setting framework I like to use is SMART, which is an acronym that stands for Specific, Measurable, Achievable, Relevant, and Time-bound. When we set a goal for ourselves, it should meet all these criteria.\nSpecific means it should be something really concrete. In this case, we don’t want to be vague (unlike when thinking about models!). What is the exact thing you’re trying to achieve? “Get better at coding” isn’t exactly specific. Perhaps instead you would like to improve your ability to recall function names that you use routinely and not need to look up the help files for them every time? Or perhaps you want to become much better at generating neat and reproducible scripts? Be as specific as possible.\nMeasurable means you can actually quantify how much improvement you make. Let’s use the specific example of being able to use some routine functions without looking at help files. Perhaps you want to be able to use at least 50 simple functions without looking up help files. That means those are functions that are part of your new vocabulary as you learn R as a language. What you’re doing with the measurable aspect of your goal is making it possible to quantify progress towards meeting your goal.\nAchievable means you think it is actually possible to complete your goal. You want to set ambitious goals for yourself, but you don’t want to make them so lofty that you’re setting yourself up for failure. If you’ve already got quite a bit of coding experience, for example, maybe you actually want to be able to use 200 routine R functions without checking help files. If you’ve never touched R in your life, that’s probably too many, and you should set a more achievable goal. Going back to an example from the first time I taught this course, one person’s achievements during the semester included being able to read in different types of data files and that was a huge accomplishment. What is achievable and ambitious for one person is not always the same for another, and you should set goals only based on your expectations for yourself, not others.\nRelevant is kinda just here to fill a spot in the acronym so it doesn’t spell SMAT. Don’t pick stupid goals. It’s up to you what that means. I would probably aim for stuff that’s going to help you in your career, but you could have other goals.\nTime-bound is key for knowing if you actually accomplished your goals. If we don’t set deadlines for ourselves, we don’t know if we succeeded. In this course, the time constraint is set for you, because we will be doing a self-assessment of our progress at the end of the course. Your goals are things you want to accomplish this semester. Outside the context of this course, however, you should think about what time frames match the goals you are setting. Keep in mind that you may have some smaller goals which get you closer to reaching a bigger goal down the road.\nAssignment: Using the SMART framework, identify 5-10 goals you have for this course. What do you aim to achieve by Week 15? What progress would you like to make? What would success look like to you at the end of this course? These could be related to coding,modeling, mathematics, comfort with equations, data management, computer literacy, research progress, statistical literacy, self-confidence, etc. Be ambitious, but realistic in what you can achieve. Your self-assessment of progress towards reaching your self-determined goals counts for 10% of your grade in this course because I want you to decide what you want to get out of the course and what will be most useful to you in your academic career. Due January 22"
  },
  {
    "objectID": "variables.html#horse-or-laminator",
    "href": "variables.html#horse-or-laminator",
    "title": "3  Random Variables",
    "section": "3.1 Horse or Laminator",
    "text": "3.1 Horse or Laminator\nToday we are talking about random variables. There is a classic scene in the absurdist play Rosencrantz and Guildenstern are Dead by Tom Stoppard in which they improbably flip a coin and it turns up heads 92 times in a row. If you flip a coin, what is the probability that it lands up heads? There is a 0.50 probability, or 50% chance, of flipping heads. There is also a 0.5 probability that the outcome is tails. These two probabilities have to sum to 1 because there is no chance of any other outcome. Put another way, if \\(p_h = 0.5\\), then \\(p_t = 1 - p_h = 0.5\\), where \\(p_h\\) is probability of heads and \\(p_t\\) is probability of tails.\nNow, instead of thinking about flipping a coin as a 50/50 shot at heads or tails, think about it only in terms of succeeding at flipping heads. Each successful attempt you get a ‘1’ for a success, and each time the coin is not heads, we call it a ‘0’ for failure. Put another way, we ask the question ‘Did we flip heads?’ and if the answer is TRUE, and we use a 1 to denote that. This is actually how R encodes TRUE (T = 1) and FALSE (F = 0).\n\n# to leave notes to yourself in R scripts, start with a #\n?rbinom # look up the Help file to find out about the arguments to the function\n\n# rbinom wants:\n# n, the number of observations / replicates\n# size, the number of draws\n# p the probability of success\n\n# to simulate a single coin flip, we want to take one replicate (n=1) of \n# one draw (size=1) from a binomial distribution  with a probability of \n# success (heads) of 0.5\nrbinom(n=1, size=1, prob=0.5)\n\n[1] 1\n\n# we could keep running this line over and over\n# or we could increase n, the number of replicates\n# this gives us a vector of 100 observations of a coin flip\nrbinom(n=100, size=1, p=c(0.5))\n\n  [1] 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n [38] 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n [75] 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0\n\n# to visualize the outcome, we can create a histogram plotting the results\n# using the function hist()\nhist(rbinom(n=100, size=1, p=0.5))\n\n\n\n# because the function is drawing from a probability distribution, outcomes will\n# be a little different every single time. If you need them to be the same, you \n# can set the seed with set.seed(42), or any other number\n\nThe outcome we are observing, successfully flipping heads, is a random variable. A random variable is one that is drawn from a probability distribution (not in the colloquial sense of the word random). Our observed response (heads = TRUE) is drawn from a Bernoulli distribution with a probability of success of 0.5. We can write this mathematically as: \\[heads \\sim Bernoulli(p=0.5)\\]\nThe Bernoulli distribution is a probability distribution that only yields success (1) or failure (0) based on the probability of success \\(p\\). The probability can vary depending on the process you are modeling, but the outcome will always be 0 or 1.\nWhile we could be very high-brow in this course and simulate the probability of that series of events, instead we will consider the remarkable case of Jo Brand’s performance in a game of ‘Horse or Laminator’ on Taskmaster. In this task, five comedians have to guess correctly whether the taskmaster has selected a card with a horse on it, or a laminator on it. For the sake of this exercise, we will assume that the taskmaster is random in his selections, and that the only thing we need to be concerned with is the probability of a comedian correctly guessing if he has selected a horse or a laminator (i.e., he is not more or less likely to choose a horse or a laminator, and there is no higher chance of a comedian being correct in the case of him selecting one or the other).\n\n\n\nHorse or Laminator\n\n\nThere are only two options, so we assume that comedians have a 50/50 shot at being correct, meaning they have a 0.5 probability of successfully guessing if the taskmaster has chosen a horse or a laminator. We can represent ‘success’ with a 1 and ‘failure’ with a 0. If the taskmaster chooses a horse and they select horse, that’s a 1; if he selects a horse and they select a laminator, that’s a 0; if he selects a laminator and they select a laminator, that’s a 1; and if he selects a laminator and they select a horse, that’s a 0. It does not matter what is selected, so long as their answers match.\nIn R, we can represent this scenario as arising from a binomial distribution. We’ve encountered the function rbinom before when playing around with code, and that is the way to randomly draw from a binomial distribution in R. For a single guess, we will make one draw from a binomial distribution of size 1, and a probability of 0.5.\n\nrbinom(n = 1, size = 1, 0.5)\n\n[1] 0\n\n\nNote that the value we get from this can only ever be 0 or 1, but, the value returned is random. We don’t always get a 1, and we don’t always get a 0, but every single time we have a 0.5 probability of getting a 1 (i.e., a ‘success’).\nNow let’s assume that each comedian has 10 guesses. That’s not actually what happens on the show, but let’s run with it for a minute. We can adjust our code so that instead of drawing one number from the distribution once, we draw one number ten times.\n\n# take ten draws of size 1 from the distribution\nrbinom(n = 10, size=1, prob=0.5)\n\n [1] 1 0 1 1 1 0 1 0 1 1\n\n# take 1 draw (for one comedian) who gets 10 guesses, and report the number of successes in those 10 attempts\nrbinom(n = 1, size=10, prob=0.5)\n\n[1] 5\n\n\nWe could now tally up the number of times a comedian was successful based on this random draw. Over the long-run, this number should balance out to roughly 50/50, but on any given draw it is random. The reason it starts to balance out when we take enough draws is due to the law of large numbers. If we make enough draws from a probability distribution, eventually it will start to resemble the expected value. Let’s try still letting our comedian guess 10 times, but now we will iterate that for 1000 attempts. So in our simulated world, you could think of it like 1000 comedians all get a shot to guess horse or laminator 10 times each, and we want to know the average number of times they were correct. We would expect it to stabilize probably somewhere around five out of 10 times.\n\n# this will give us 1000 simulations of 10 draws from binomial\nrbinom(n = 1000, size=10, prob=0.5)\n\n   [1]  7  4  3  5  4  3  4  6  5  5  6  4  4  4  4  5  4  5  5  4  6  4  4  4\n  [25]  5  5  4  6  3  4  2  8  3  6  7  7  6  5  3  4  3  6  5  3  4  5  3  2\n  [49]  4  4  6  4  3  3  6  2  5  6  3  5  7  7  5  4  3  6  3  6  7  5  7  5\n  [73]  5  6  8  3  3  5  4  7  7  5  5  4  6  4  5  6  6  4  7  4  2  6  7  3\n  [97]  7  5  3  4  7  6  6  7  7  8  7  5  6  3  7  5  6  5  5  3  2  8  6  3\n [121]  5  4  5  5  3  6  6  5  6 10  8  4  3  5  5  6  6  6  6  6  4  7  6  3\n [145]  6  5  3  5  3  6  5  8  8  4  4  7  4  5  4  3  6  6  5  5  5  6  3  6\n [169]  4  4  7  8  4  7  7  3  3  7  7  7  4  6  4  6  6  3  8  7  3  5  4  5\n [193]  8  2  3  2  3  3  2  5  4  2  7  3  7  6  4  5  5  6  2  4  4  4  9  4\n [217]  7  7  4  6  5  3  5  4  3  6  6  3  7  3  5  6  5  6  6  6  3  6  5  5\n [241]  5  5  6  6  4  3  3  7  5  5  4  5  2  3  4  3  6  5  4  9  5  6  4  4\n [265]  6  4  5  7  4  4  3  5  3  4  5  4  5  5  5  2  5  4  6  4  4  5  6  5\n [289]  3  6  8  7  5  5  6  2  4  4  7  7  6  3  4  6  7  4  7  5  0  6  4  3\n [313]  4  5  4  8  6  6  5  7  4  6  4  6  6 10  5  5  4  6  5  4  5  6  3  4\n [337]  0  6  3  4  5  5  7  6  3  4  4  4  4  8  4  5  7  4  6  5  7  2  4  5\n [361]  7  5  4  5  6  2  5  5  2  4  4  4  6  2  4  4  4  8  7  4  4  4  7  8\n [385]  3  5  6  4  8  6  4  3  5  4  3  6  4  3  6  4  5  3  4  3  5  7  6  7\n [409]  5  3  8  6  3  5  4  6  4  6  4  4  5  4  6  3  6  8  6  4  5  7  7  4\n [433]  7  5  6  6  3  3  6  7  7  8  1  7  5  6  6  5  4  7  7  5  3  5  5  5\n [457]  5  4  7  3  4  5  7  6  5  6  5  5  5  5  4  2  6  5  5  4  7  7  2  4\n [481]  5  6  4  5  7  3  7  7  6  7  6  6  6  2  6  6  6  4  5  6  5  5  4  3\n [505]  5  5  3  5  4  5  3  3  4  6  3  3  6  6  6  5  7  8  3  6  4  4  1  7\n [529]  5  2  6  7  6  5  4  3  4  4  4  6  7  6  5  7  7  6  4  4  7  2  5  5\n [553]  4  5  5  3  6  4  7  3  4  3  5  8  6  4  5  5  5  5  4  6  4  6  4  8\n [577]  5  4  6  7  9  4  5  9  4  7  7  5  6  5  2  2  5  6  5  4  3  7  4  5\n [601]  4  4  6  5  9  5  5  4  5  6  6  6  4  7  2  7  5  4  4  5  4  2  3  3\n [625]  3  2  6  5  5  3  5  4  7  1  4  8  5  6  5  5  2  4  6  6  6  8  6  2\n [649]  6  5  5  6  4  6  5  5  8  9  6  4  5  6  5  6  4  5  6  4  9  6  5  5\n [673]  7  6  3  3  6  3  7  6  6  7  7  6  6  5  4  7  6  5  5  7  5  3  3  5\n [697]  9  4  8  6  6  6  4  8  8  6  4  4  4  5  5  6  5  4  5  6  7  5  5  7\n [721]  5  4  8  5  5  6  5  6  4  6  4  5  6  4  2  6  8  2  3  4  4  6  7  6\n [745]  4  6  2  6  4  5  2  3  4  6  4  7  3  5  5  5  7  6  5  4  3  5  4  5\n [769]  3  4  5  8  3  9  8  4  3  6  4  5  4  6  7  4  7  3  3  3  5  5  5  1\n [793]  4  7  3  3  6  5  3  6  2  3  4  7  6  6  5  4  9  6  5  6  3  5  4  6\n [817]  5  6  2  6  8  6  4  7  6  6  4  6  6  4  6  5  6  4  5  3  6  3  6  6\n [841]  7  6  3  7  4  8  5  5  3  4  3  5  3  6  4  5  7  4  4  8  6  6  8  6\n [865]  6  4  5  5  8  3  5  5  4  4  5  6  6  5  6  3  8  5  2  7  8  5  7  7\n [889]  3  5  4  3  7  3  5  6  8  7  6  6  2  4  4  1  7  7  7  6  6  5  4  6\n [913]  5  4  7  7  4  2  6  5  5  5  4  5  4  4  4  4  6  5  8  2  5  3  5  5\n [937]  2  4  7  4  5  7  6  7  7  4  5  3  6  3  6  8  7  7  5  6  5  7  4  4\n [961]  6  5  3  6  4  6  2  5  3  7  4  4  3  2  4  6  5  5  5  7  4  2  7  4\n [985]  5  6  7  5  7  2  5  4  5  3  6  5  6  3  4  6\n\n# we should save this to an object so it is easier to work with\nten_attempts &lt;- rbinom(n = 1000, size = 10, prob=0.5)\ntable(ten_attempts)\n\nten_attempts\n  1   2   3   4   5   6   7   8   9  10 \n 11  41 119 207 242 200 122  49   7   2 \n\nhist(ten_attempts)\n\n\n\n\nNow, what actually happens on Taskmaster is that the first comedian gets five correct in a row (horse, horse, horse, horse, laminator). The second gets one correct. The third gets zero correct, the fourth gets one, and the final comedian, Jo Brand, guesses horse or laminator correct 13 times in a row. Using the rbinom function, work out what the probability of each of these events is. Keep in mind that now instead of getting a certain number of guesses and tallying success across all those, as soon as a comedian guesses incorrectly, they are out. So we are looking only for that number of successful events in a row to calculate the probability.\n\n# recode david's as 1, because otherwise length doesn't work correctly; you can't guess 0 times\n# there are many other solutions possible\n# you could subset stuff, for example\n# or use logical vectors to find true and false, etc.\nscores &lt;- c(5, 1, 1, 1, 13)\n\nfor(i in scores){\n  print(prop.table(table(rbinom(n = 1000000, size = i, prob=0.5)==i)))\n}\n\n\n   FALSE     TRUE \n0.968634 0.031366 \n\n   FALSE     TRUE \n0.500333 0.499667 \n\n   FALSE     TRUE \n0.499739 0.500261 \n\n   FALSE     TRUE \n0.499226 0.500774 \n\n   FALSE     TRUE \n0.999878 0.000122 \n\n\n## Guillemot chick survival\nTo move on to a slightly more ecological example than coin flips or horses and laminators, let’s think about guillemots. Guillemots are seabirds that nest on steep cliffsides to escape predators. They spend most of their life on the ocean, however, and as such are very awkward on land and also not great at flying. To get from the cliffside nest to the ocean for the first time, guillemot chicks have to jump and then glide/fly. If they do not make it all the way, they can attempt to run, but are vulnerable to predation. The only option for chicks is to survive or fail - making it 70% of the way to the ocean will result in failure (and, in this case, death). Thus the probability of success does not equate to the actual rate of success - it is the probability per trial (i.e., each attempt).\n\n\n\n# let's assume chicks have a 40% chance of making it to the ocean, and there are\n# 100 chicks jumping from the cliff hoping to make it to the ocean so we have \n# 100 observations, each with one draw from the Bernoulli distribution\nrbinom(n = 100, size=1, p=0.4)\n\n  [1] 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0\n [38] 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n [75] 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n\n# so far, we have just been running functions and the output is shown in the console\n# but we don't have anything in our environment\n# can save to an object with the &lt;- assignment operator\n\nsurvival &lt;- rbinom(n=100, size=1, p=0.4)\n\n# we can then do things with that saved object, like plot it\nhist(survival)"
  },
  {
    "objectID": "variables.html#binomial-distribution",
    "href": "variables.html#binomial-distribution",
    "title": "3  Random Variables",
    "section": "3.2 Binomial distribution",
    "text": "3.2 Binomial distribution\n\n3.2.1 Guillemot example\nGuillemots typically only lay 1-2 eggs, but for the sake of our example code, let’s pretend that each pair has 3 chicks that are attempting to make it to the ocean. Each individual chick’s attempt could be modeled with a Bernoulli distribution, but if we consider it from the perspective of the breeding pair, the total number of surviving offspring is what matters. We can think of the number of chicks from each nest that make it to the ocean as the sum of their individual attempts for a single nest. This is the Binomial distribution, which is a more generalized version of the Bernoulli distribution (more accurately, the Bernoulli distribution is a special case of the Binomial distribution with one draw) in which each observation has multiple draws.\n\n# for one nest with 3 chicks and a probability of success of 0.4\nrbinom(n=1, size=3, p=0.4)\n\n[1] 2\n\n# we could also think about a colony with let's say 100 nests\n# how many chicks from each nest will make it?\nnest_offspring &lt;- rbinom(n=100, size=3, p=0.4)\nhist(nest_offspring)\n\n\n\nnest_offspring\n\n  [1] 0 1 1 2 1 1 2 1 2 2 1 1 1 0 2 1 0 0 2 2 1 0 3 1 0 1 0 1 0 2 0 1 1 0 1 2 2\n [38] 2 1 1 1 1 2 1 2 3 2 3 1 1 0 1 0 2 0 1 2 1 0 1 1 1 2 3 1 1 2 0 0 2 1 2 1 1\n [75] 2 1 0 3 1 1 0 1 2 3 2 1 1 1 1 2 2 2 1 0 1 2 2 2 0 1\n\n# what is the probability that all three chicks from a nest make it?\n# because we know what we fixed p to, we could multiply the probabilities\n0.4^3\n\n[1] 0.064\n\n# what is the probability that none make it?\n(1 - 0.4)^3\n\n[1] 0.216\n\n\nWe are able to calculate what the probability of these outcomes are because we fixed the probability of success at 0.4 and then simulated outcomes. In the real world, we rarely know what the probability of success truly is, and instead we are working backwards from our observed outcomes (i.e. our data) to estimate the probability of survival. One of the beautiful things about working with simulated data is that we know the input value, so we can test assumptions, see if we recapture our input, and play around with different model structures and know that anything that is unexpected is most likely a problem with our code or model, not the data.\nWith discrete distributions like the Binomial, we can convert the frequency of outcomes to a proportion of the total as an estimate of probability. Dividing the frequencies by the total sample size makes it so that the total probability sums to 1.\n\n# table is a function that tallies up all the items in a vector\n# a vector is a type of object that is one dimension, i.e. nest_offspring is a \n# vector with length = 100\ntable(nest_offspring)\n\nnest_offspring\n 0  1  2  3 \n20 45 29  6 \n\n# proportions will convert the table into proportions instead of counts\n# so will prop.table and I often use that function because I learned it first\n# there are many different ways to accomplish the same goal in R\nsurvival_prob &lt;- proportions(table(nest_offspring))\n\n# How do these values compare to the calculated probability of all three chicks\n# surviving? What about of none surviving?\n\n# With the values calculated from the simulated data (i.e. not the parameter\n# value that we fixed), what is the probability that *at least* one chick \n# from a nest survives?\n0.47 + 0.27 + 0.06 # note: numbers might be different because we didn't set.seed()\n\n[1] 0.8\n\n\nIn general with coding, you want to avoid hard coding like this where you put in fixed values. It is much better to use code that is flexible if the input data changes (e.g. we are using a randomly generated dataset, so it will change every time), or if you want to change some parameter across a lot of different parts of the code. For example, instead of repeatedly typing p=0.4, we could have created an object in our environment with the probability of success such as p.surv &lt;- 0.4 and then specifying p=p.surv throughout the script, which would make it really easy to change the simulations for a new scenario (e.g. if the guillemot chicks were given little hang-gliders, we might increase p.surv and would only have to type it out once as p.surv &lt;- 0.8.\nSubsetting vectors in R is a really useful tool when you want to apply a function to only part of a vector, inspect part of an object, etc. We use the square brackets [] for subsetting, and within them specify which elements to return. The elements to return can be a numeric vector (e.g. c(1,2,4)) to return the first, second, and fourth elements, or a logical vector indicating if an element should or should not be returned (e.g. c(TRUE TRUE FALSE TRUE)). If subsetting a range of consecutive elements, the : operator can also be used (e.g. 1:4 is the same as c(1,2,3,4)).\n\n# To avoid hard coding our estimated percent, we can use sum() on a subset of\n# the table of proportions to estimate the probability at least one survives?\nsum(survival_prob[2:4])\n\n[1] 0.8\n\n# what is the probability at least two survive?\nsum(survival_prob[3:4])\n\n[1] 0.35\n\n# side note: if you're not sure which indices to subset, it can help to look at\n# your object; you can easily do this in RStudio by highlighting just the bit of\n# code with your object name and using Ctrl + Enter (or Cmd + Enter) to print\n# just that object to the console, i.e. if you highlight part of a line, you will\n# only run the highlighted bit, not the entire line\n\n\n\n3.2.2 Coral example\nWith the guillemots, we were assuming that each pair had three potential offspring surviving. What happens if we vastly increase the number of draws from the binomial distribution? Take corals as an example. Some broadcast spawning corals have mass synchronized spawning events where individual corals on a reef all release bundles of sperm and eggs at the same time. The reproductive success of each individual coral can still be modeled as a binomial distribution, i.e. how many of those released sperm and eggs will actually encounter a bundle of the same species and be a successful mating attempt is a random variable drawn from a Binomial distribution with a number of draws \\(N\\) equal to the number of released sperm and eggs and a probability of success \\(p\\) for each of those. \\[ offspring \\sim Binomial(N, p)\\].\nNote: the \\(N\\) for denoting the number of draws from the Binomial is distinct from the n = argument in the rbinom() function. Don’t let this confuse you! \\(N\\) equates to the size = argument, while n = is the number of observations of the random variable, i.e. the total number of individual corals on the reef.\n\n\n\n# let's assume now the coral reef has 100,00 individual corals (I have no \n# clue if this is an accurate number, but let's roll with it)\n# let's also assume each individual coral releases 10,000 sperm and eggs\n# and the probability for each of those resulting in a successful mating event \n# 0.0002 (i.e. 0.002%)\n\ngametes &lt;- rbinom(n=100000, size=10000, p=0.0002)\nhist(gametes, breaks = 100)\n\n\n\n# what is the probability of reproductive failure?\nprop.table(table(gametes))\n\ngametes\n      0       1       2       3       4       5       6       7       8       9 \n0.13325 0.27077 0.27168 0.18012 0.09020 0.03723 0.01194 0.00361 0.00082 0.00029 \n     10 \n0.00009 \n\n# what's the probability of more than 5 offspring?\nprop.table(table(gametes&gt;5))\n\n\n  FALSE    TRUE \n0.98325 0.01675 \n\n# now let's increase the number of individual corals to 1 million, and also increase\n# the probability of success to 0.001 \n\ngametes2 &lt;- rbinom(n=1000000, size=10000, p=0.001)\nhist(gametes2, breaks=100)\n\n\n\n# now what is the probability of reproductive failure?\nprop.table(table(gametes2==0))\n\n\n   FALSE     TRUE \n0.999955 0.000045 \n\n# and what is the probability of more than 15 offspring?\nprop.table(table(gametes2&gt;15))\n\n\n   FALSE     TRUE \n0.951431 0.048569 \n\n# let's visualize the data as a histogram again, but add a vertical bar using \n# the abline() function at 15; note there are other plotting options to make \n# your plots slightly more aesthetic\nhist(gametes2, \n     breaks=100,\n     border=F, # i just don't like borders on bars\n     col=\"turquoise4\", \n     main=\"\", # removes the 'title'\n     xlab=\"Number of successful mating attempts\", # changes x axis label\n     las=1 # rotates the axis labels the right way\n     )\nabline(v=15, \n       lwd=2, # width of line\n       col=\"tomato2\")"
  }
]